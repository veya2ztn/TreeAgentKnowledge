{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da1706d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os,json\n",
    "base_url = \"https://api.gptsapi.net/v1\";model    = \"gpt-4o-mini\" \n",
    "# base_url = \"https://api.deepseek.com/beta\";model    = \"deepseek-chat\" \n",
    "with open(\".openai.conf.json\",'r') as f:\n",
    "    openai_config = json.load(f)\n",
    "api_key  = openai_config[base_url]['api_key'] #\n",
    "\n",
    "import os\n",
    "for var in \"http_proxy https_proxy HTTP_PROXY HTTPS_PROXY\".split():\n",
    "    os.environ[var]=\"\"\n",
    "\n",
    "base_url = \"http://10.140.24.65:30000/v1\"\n",
    "\n",
    "max_tokens= 16384\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']=api_key\n",
    "os.environ['OPENAI_API_BASE_URL']=base_url\n",
    "from tree_operations import *\n",
    "\n",
    "with open('extra_content.txt','r') as f:\n",
    "    content = f.read()\n",
    "tree = NodeTree.load_from_file(\"TreeKnowledge.example.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c68dbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m choose_pool \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m(i):k \u001b[38;5;28;01mfor\u001b[39;00m i,k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mchildren_keys)}\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@sgl\u001b[39m\u001b[38;5;241m.\u001b[39mfunction\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecide_next_position\u001b[39m(s):\n\u001b[1;32m      4\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sgl\u001b[38;5;241m.\u001b[39muser(system_prompt\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNow lets assume it is LOWER, please decide the next position for the content:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "choose_pool = {str(i):k for i,k in enumerate(self.children_keys)}\n",
    "@sgl.function\n",
    "def decide_next_position(s):\n",
    "    s += sgl.user(system_prompt+\"\\n\"+\"Now lets assume it is LOWER, please decide the next position for the content:\")\n",
    "    s += sgl.assistant(\n",
    "        sgl.gen(\n",
    "            \"answer\",\n",
    "            choices=list(choose_pool.keys()),\n",
    "            choices_method=sgl.greedy_token_selection,\n",
    "        )\n",
    "    )\n",
    "next_position = decide_next_position.run()['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b99fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bed912a75c4e9d88fe985af606e6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Content_root=\"custom_collection/graphrag/whole/summary_with_logic_position\"\n",
    "contents = []\n",
    "for i,filename in enumerate(tqdm(os.listdir(Content_root))):\n",
    "    filepath = os.path.join(Content_root,filename )\n",
    "    with open(filepath,'r') as f:\n",
    "        content = f.read()\n",
    "    contents.append(Content(i,content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497fbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.shuffle(contents)\n",
    "contents_split = np.array_split(contents,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c92a7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4c7f22610d4b1eb16dfd93333d987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> MixMIM Methodology -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> MixMIM Methodology\n",
      "<- Action: UPDATE at MixMIM Methodology\n",
      "<- Action: ADD at MixMIM Methodology\n",
      "<- Action: ADD at Image Captioning and Scene Text Recognition\n",
      "<- Action: UPDATE at End-to-End Object Detectors\n",
      "<- Action: ADD at End-to-End Object Detectors\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: UPDATE at Decoupling Detection and Classification\n",
      "<- Action: UPDATE at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n"
     ]
    }
   ],
   "source": [
    "for content_list in tqdm(contents_split[:1]):\n",
    "    already_passed_paper = set(tree.get_whole_represented_pool())\n",
    "    content_list = [content for content in content_list if int(content.id_address) not in tree.get_whole_represented_pool()]\n",
    "    await tree.dispatch_multiple_contents(list(content_list))\n",
    "    await tree.backpropagate(do_evolve=True)\n",
    "    tree.save_to_json(\"sglang.snap.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e0afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbb = tree.retrieve_all_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf81b21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>old_concept_key_word</th>\n",
       "      <th>old_concept_abstract</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>action</th>\n",
       "      <th>new_concept_key_word</th>\n",
       "      <th>new_concept_abstract</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Image Captioning and Scene Text Recognition</td>\n",
       "      <td>Image Captioning and Scene Text Recognition</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-21-49-681198</td>\n",
       "      <td>add</td>\n",
       "      <td>Speculative Jacobi Decoding</td>\n",
       "      <td>Speculative Jacobi Decoding (SJD) is a novel, ...</td>\n",
       "      <td>SJD represents a distinct specialized method a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-21-50-159700</td>\n",
       "      <td>add</td>\n",
       "      <td>Augmented Shortcuts</td>\n",
       "      <td>Augmented Shortcuts are a novel method introdu...</td>\n",
       "      <td>Augmented shortcuts represent a distinct appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-21-50-178737</td>\n",
       "      <td>add</td>\n",
       "      <td>Scale-Steered Convolutional Networks</td>\n",
       "      <td>Scale-Steered Convolutional Networks (SS-CNNs)...</td>\n",
       "      <td>The introduction of SS-CNNs represents a disti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>Image Captioning and Scene Text Recognition</td>\n",
       "      <td>Recent approaches in image captioning and scen...</td>\n",
       "      <td>2024-11-02-16-21-51-113554</td>\n",
       "      <td>update</td>\n",
       "      <td>Recent approaches in image captioning and scen...</td>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>To better reflect the expanded knowledge struc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dynamic Token Optimization Techniques</td>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td>A framework that dynamically prunes tokens bas...</td>\n",
       "      <td>2024-11-02-16-21-54-301164</td>\n",
       "      <td>update</td>\n",
       "      <td>Dynamic Token Sparsification is a framework th...</td>\n",
       "      <td>Dynamic Token Optimization Techniques</td>\n",
       "      <td>To reflect the broader category of techniques ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Augmented Shortcuts</td>\n",
       "      <td>Augmented Shortcuts</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-22-466551</td>\n",
       "      <td>append</td>\n",
       "      <td>Paper 1869</td>\n",
       "      <td></td>\n",
       "      <td>The new content enhances the existing concept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vision-Language Models</td>\n",
       "      <td>Vision-Language Models</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-23-014679</td>\n",
       "      <td>add</td>\n",
       "      <td>MagicMix</td>\n",
       "      <td>MagicMix is a novel semantic mixing technique ...</td>\n",
       "      <td>MagicMix introduces a distinct technique for s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>Augmented Shortcuts</td>\n",
       "      <td>Augmented Shortcuts are a novel method introdu...</td>\n",
       "      <td>2024-11-02-16-22-23-806545</td>\n",
       "      <td>update</td>\n",
       "      <td>Augmented Shortcuts are a novel method introdu...</td>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>To reflect the broader context of augmented sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vision-Language Techniques</td>\n",
       "      <td>Vision-Language Models</td>\n",
       "      <td>Models that integrate visual and textual data ...</td>\n",
       "      <td>2024-11-02-16-22-24-598324</td>\n",
       "      <td>update</td>\n",
       "      <td>Vision-Language Models integrate visual and te...</td>\n",
       "      <td>Vision-Language Techniques</td>\n",
       "      <td>To reflect the expanded knowledge structure th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>Augmented Shortcuts are a novel method introdu...</td>\n",
       "      <td>2024-11-02-16-22-25-515673</td>\n",
       "      <td>update</td>\n",
       "      <td>Advanced neural network techniques, including ...</td>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>The update incorporates detailed information a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Literature Research Report</td>\n",
       "      <td>Literature Research Report</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-27-291699</td>\n",
       "      <td>add</td>\n",
       "      <td>Temporal Span Proposal Network</td>\n",
       "      <td>Temporal Span Proposal Network (TSPN) is a nov...</td>\n",
       "      <td>The TSPN represents a distinct specialized app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Advanced Learning and Detection Techniques</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-29-476492</td>\n",
       "      <td>update</td>\n",
       "      <td>Advancements in Self-Supervised Learning, Obje...</td>\n",
       "      <td>Advanced Learning and Detection Techniques</td>\n",
       "      <td>To reflect the expanded knowledge structure th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-Supervised Learning (SSL) Techniques</td>\n",
       "      <td>Self-Supervised Learning (SSL) Techniques</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-34-344804</td>\n",
       "      <td>append</td>\n",
       "      <td>Paper 1503</td>\n",
       "      <td></td>\n",
       "      <td>The new content enhances the existing concept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-34-460241</td>\n",
       "      <td>add</td>\n",
       "      <td>ColorFormer</td>\n",
       "      <td>ColorFormer is a transformer-based architectur...</td>\n",
       "      <td>ColorFormer introduces a distinct approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Temporal Span Proposal Network</td>\n",
       "      <td>Temporal Span Proposal Network</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-34-765169</td>\n",
       "      <td>add</td>\n",
       "      <td>Sparse Proposals with Hierarchical Features</td>\n",
       "      <td>Sparse Proposals with Hierarchical Features (S...</td>\n",
       "      <td>SP-TAD represents a distinct approach to Tempo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>Temporal Span Proposal Network</td>\n",
       "      <td>Temporal Span Proposal Network (TSPN) is a nov...</td>\n",
       "      <td>2024-11-02-16-22-36-597204</td>\n",
       "      <td>update</td>\n",
       "      <td>Temporal Span Proposal Network (TSPN) is a nov...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>To reflect the inclusion of the child node tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>Self-Supervised Learning (SSL) Techniques</td>\n",
       "      <td>Self-supervised learning has become a prominen...</td>\n",
       "      <td>2024-11-02-16-22-37-030887</td>\n",
       "      <td>update</td>\n",
       "      <td>Self-supervised learning has become a prominen...</td>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>The update reflects the inclusion of generativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>Recent approaches in image captioning and scen...</td>\n",
       "      <td>2024-11-02-16-22-37-625815</td>\n",
       "      <td>update</td>\n",
       "      <td>Recent approaches in image captioning and scen...</td>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>To incorporate the newly added ColorFormer tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>Self-supervised learning has become a prominen...</td>\n",
       "      <td>2024-11-02-16-22-39-195500</td>\n",
       "      <td>update</td>\n",
       "      <td>Self-supervised learning techniques are increa...</td>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>The update incorporates new insights into self...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>Temporal Span Proposal Network (TSPN) is a nov...</td>\n",
       "      <td>2024-11-02-16-22-39-323019</td>\n",
       "      <td>update</td>\n",
       "      <td>The Temporal Span Proposal Network (TSPN) is a...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>The update incorporates enhanced clarity and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Point Transformer Design</td>\n",
       "      <td>Point Transformer Design</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-46-459857</td>\n",
       "      <td>append</td>\n",
       "      <td>Paper 1030</td>\n",
       "      <td></td>\n",
       "      <td>The new content enhances the existing concept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CNN Encoder and Decoder Design</td>\n",
       "      <td>CNN Encoder and Decoder Design</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-46-752473</td>\n",
       "      <td>add</td>\n",
       "      <td>Character-Aware Neural Network</td>\n",
       "      <td>Char-Net is a character-aware neural network d...</td>\n",
       "      <td>Char-Net represents a distinct approach within...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-46-933679</td>\n",
       "      <td>add</td>\n",
       "      <td>Progressive Autoregressive Video Diffusion Models</td>\n",
       "      <td>Progressive Autoregressive Video Diffusion Mod...</td>\n",
       "      <td>The new content introduces a distinct approach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>Point Transformer Design</td>\n",
       "      <td>Utilizes self-attention networks to process po...</td>\n",
       "      <td>2024-11-02-16-22-47-908857</td>\n",
       "      <td>update</td>\n",
       "      <td>Point Transformer Design utilizes self-attenti...</td>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>To reflect the broader application and techniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CNN Design for Text Recognition</td>\n",
       "      <td>CNN Encoder and Decoder Design</td>\n",
       "      <td>Uses a ResNet34-based CNN encoder and an atten...</td>\n",
       "      <td>2024-11-02-16-22-48-326479</td>\n",
       "      <td>update</td>\n",
       "      <td>CNN Encoder and Decoder Design utilizes a ResN...</td>\n",
       "      <td>CNN Design for Text Recognition</td>\n",
       "      <td>The update reflects the integration of charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Video Visual Relation Detection and Generation...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>The Temporal Span Proposal Network (TSPN) is a...</td>\n",
       "      <td>2024-11-02-16-22-48-782653</td>\n",
       "      <td>update</td>\n",
       "      <td>The Temporal Span Proposal Network (TSPN) is a...</td>\n",
       "      <td>Video Visual Relation Detection and Generation...</td>\n",
       "      <td>To reflect the inclusion of Progressive Autore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>Point Transformer Design utilizes self-attenti...</td>\n",
       "      <td>2024-11-02-16-22-50-330780</td>\n",
       "      <td>update</td>\n",
       "      <td>Point cloud processing involves the use of sel...</td>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>The update incorporates a more comprehensive u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               parent  \\\n",
       "16        Image Captioning and Scene Text Recognition   \n",
       "7                        Dynamic Token Sparsification   \n",
       "8                        Dynamic Token Sparsification   \n",
       "17        Image Captioning and Recognition Techniques   \n",
       "9               Dynamic Token Optimization Techniques   \n",
       "10                                Augmented Shortcuts   \n",
       "5                              Vision-Language Models   \n",
       "11                 Advanced Neural Network Techniques   \n",
       "6                          Vision-Language Techniques   \n",
       "12                 Advanced Neural Network Techniques   \n",
       "0                          Literature Research Report   \n",
       "1          Advanced Learning and Detection Techniques   \n",
       "2           Self-Supervised Learning (SSL) Techniques   \n",
       "18        Image Captioning and Recognition Techniques   \n",
       "22                     Temporal Span Proposal Network   \n",
       "23  Temporal Span Proposal Network and Action Dete...   \n",
       "3                 Self-Supervised Learning Techniques   \n",
       "19        Image Captioning and Recognition Techniques   \n",
       "4                 Self-Supervised Learning Techniques   \n",
       "24  Temporal Span Proposal Network and Action Dete...   \n",
       "13                           Point Transformer Design   \n",
       "20                     CNN Encoder and Decoder Design   \n",
       "25  Temporal Span Proposal Network and Action Dete...   \n",
       "14                             Point Cloud Processing   \n",
       "21                    CNN Design for Text Recognition   \n",
       "26  Video Visual Relation Detection and Generation...   \n",
       "15                             Point Cloud Processing   \n",
       "\n",
       "                                 old_concept_key_word  \\\n",
       "16        Image Captioning and Scene Text Recognition   \n",
       "7                        Dynamic Token Sparsification   \n",
       "8                        Dynamic Token Sparsification   \n",
       "17        Image Captioning and Scene Text Recognition   \n",
       "9                        Dynamic Token Sparsification   \n",
       "10                                Augmented Shortcuts   \n",
       "5                              Vision-Language Models   \n",
       "11                                Augmented Shortcuts   \n",
       "6                              Vision-Language Models   \n",
       "12                 Advanced Neural Network Techniques   \n",
       "0                          Literature Research Report   \n",
       "1                                                       \n",
       "2           Self-Supervised Learning (SSL) Techniques   \n",
       "18        Image Captioning and Recognition Techniques   \n",
       "22                     Temporal Span Proposal Network   \n",
       "23                     Temporal Span Proposal Network   \n",
       "3           Self-Supervised Learning (SSL) Techniques   \n",
       "19        Image Captioning and Recognition Techniques   \n",
       "4                 Self-Supervised Learning Techniques   \n",
       "24  Temporal Span Proposal Network and Action Dete...   \n",
       "13                           Point Transformer Design   \n",
       "20                     CNN Encoder and Decoder Design   \n",
       "25  Temporal Span Proposal Network and Action Dete...   \n",
       "14                           Point Transformer Design   \n",
       "21                     CNN Encoder and Decoder Design   \n",
       "26  Temporal Span Proposal Network and Action Dete...   \n",
       "15                             Point Cloud Processing   \n",
       "\n",
       "                                 old_concept_abstract  \\\n",
       "16                                                      \n",
       "7                                                       \n",
       "8                                                       \n",
       "17  Recent approaches in image captioning and scen...   \n",
       "9   A framework that dynamically prunes tokens bas...   \n",
       "10                                                      \n",
       "5                                                       \n",
       "11  Augmented Shortcuts are a novel method introdu...   \n",
       "6   Models that integrate visual and textual data ...   \n",
       "12  Augmented Shortcuts are a novel method introdu...   \n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "18                                                      \n",
       "22                                                      \n",
       "23  Temporal Span Proposal Network (TSPN) is a nov...   \n",
       "3   Self-supervised learning has become a prominen...   \n",
       "19  Recent approaches in image captioning and scen...   \n",
       "4   Self-supervised learning has become a prominen...   \n",
       "24  Temporal Span Proposal Network (TSPN) is a nov...   \n",
       "13                                                      \n",
       "20                                                      \n",
       "25                                                      \n",
       "14  Utilizes self-attention networks to process po...   \n",
       "21  Uses a ResNet34-based CNN encoder and an atten...   \n",
       "26  The Temporal Span Proposal Network (TSPN) is a...   \n",
       "15  Point Transformer Design utilizes self-attenti...   \n",
       "\n",
       "                     timestamp  action  \\\n",
       "16  2024-11-02-16-21-49-681198     add   \n",
       "7   2024-11-02-16-21-50-159700     add   \n",
       "8   2024-11-02-16-21-50-178737     add   \n",
       "17  2024-11-02-16-21-51-113554  update   \n",
       "9   2024-11-02-16-21-54-301164  update   \n",
       "10  2024-11-02-16-22-22-466551  append   \n",
       "5   2024-11-02-16-22-23-014679     add   \n",
       "11  2024-11-02-16-22-23-806545  update   \n",
       "6   2024-11-02-16-22-24-598324  update   \n",
       "12  2024-11-02-16-22-25-515673  update   \n",
       "0   2024-11-02-16-22-27-291699     add   \n",
       "1   2024-11-02-16-22-29-476492  update   \n",
       "2   2024-11-02-16-22-34-344804  append   \n",
       "18  2024-11-02-16-22-34-460241     add   \n",
       "22  2024-11-02-16-22-34-765169     add   \n",
       "23  2024-11-02-16-22-36-597204  update   \n",
       "3   2024-11-02-16-22-37-030887  update   \n",
       "19  2024-11-02-16-22-37-625815  update   \n",
       "4   2024-11-02-16-22-39-195500  update   \n",
       "24  2024-11-02-16-22-39-323019  update   \n",
       "13  2024-11-02-16-22-46-459857  append   \n",
       "20  2024-11-02-16-22-46-752473     add   \n",
       "25  2024-11-02-16-22-46-933679     add   \n",
       "14  2024-11-02-16-22-47-908857  update   \n",
       "21  2024-11-02-16-22-48-326479  update   \n",
       "26  2024-11-02-16-22-48-782653  update   \n",
       "15  2024-11-02-16-22-50-330780  update   \n",
       "\n",
       "                                 new_concept_key_word  \\\n",
       "16                        Speculative Jacobi Decoding   \n",
       "7                                 Augmented Shortcuts   \n",
       "8                Scale-Steered Convolutional Networks   \n",
       "17  Recent approaches in image captioning and scen...   \n",
       "9   Dynamic Token Sparsification is a framework th...   \n",
       "10                                         Paper 1869   \n",
       "5                                            MagicMix   \n",
       "11  Augmented Shortcuts are a novel method introdu...   \n",
       "6   Vision-Language Models integrate visual and te...   \n",
       "12  Advanced neural network techniques, including ...   \n",
       "0                      Temporal Span Proposal Network   \n",
       "1   Advancements in Self-Supervised Learning, Obje...   \n",
       "2                                          Paper 1503   \n",
       "18                                        ColorFormer   \n",
       "22        Sparse Proposals with Hierarchical Features   \n",
       "23  Temporal Span Proposal Network (TSPN) is a nov...   \n",
       "3   Self-supervised learning has become a prominen...   \n",
       "19  Recent approaches in image captioning and scen...   \n",
       "4   Self-supervised learning techniques are increa...   \n",
       "24  The Temporal Span Proposal Network (TSPN) is a...   \n",
       "13                                         Paper 1030   \n",
       "20                     Character-Aware Neural Network   \n",
       "25  Progressive Autoregressive Video Diffusion Models   \n",
       "14  Point Transformer Design utilizes self-attenti...   \n",
       "21  CNN Encoder and Decoder Design utilizes a ResN...   \n",
       "26  The Temporal Span Proposal Network (TSPN) is a...   \n",
       "15  Point cloud processing involves the use of sel...   \n",
       "\n",
       "                                 new_concept_abstract  \\\n",
       "16  Speculative Jacobi Decoding (SJD) is a novel, ...   \n",
       "7   Augmented Shortcuts are a novel method introdu...   \n",
       "8   Scale-Steered Convolutional Networks (SS-CNNs)...   \n",
       "17        Image Captioning and Recognition Techniques   \n",
       "9               Dynamic Token Optimization Techniques   \n",
       "10                                                      \n",
       "5   MagicMix is a novel semantic mixing technique ...   \n",
       "11                 Advanced Neural Network Techniques   \n",
       "6                          Vision-Language Techniques   \n",
       "12                 Advanced Neural Network Techniques   \n",
       "0   Temporal Span Proposal Network (TSPN) is a nov...   \n",
       "1          Advanced Learning and Detection Techniques   \n",
       "2                                                       \n",
       "18  ColorFormer is a transformer-based architectur...   \n",
       "22  Sparse Proposals with Hierarchical Features (S...   \n",
       "23  Temporal Span Proposal Network and Action Dete...   \n",
       "3                 Self-Supervised Learning Techniques   \n",
       "19        Image Captioning and Recognition Techniques   \n",
       "4                 Self-Supervised Learning Techniques   \n",
       "24  Temporal Span Proposal Network and Action Dete...   \n",
       "13                                                      \n",
       "20  Char-Net is a character-aware neural network d...   \n",
       "25  Progressive Autoregressive Video Diffusion Mod...   \n",
       "14                             Point Cloud Processing   \n",
       "21                    CNN Design for Text Recognition   \n",
       "26  Video Visual Relation Detection and Generation...   \n",
       "15                             Point Cloud Processing   \n",
       "\n",
       "                                               reason  \n",
       "16  SJD represents a distinct specialized method a...  \n",
       "7   Augmented shortcuts represent a distinct appro...  \n",
       "8   The introduction of SS-CNNs represents a disti...  \n",
       "17  To better reflect the expanded knowledge struc...  \n",
       "9   To reflect the broader category of techniques ...  \n",
       "10  The new content enhances the existing concept ...  \n",
       "5   MagicMix introduces a distinct technique for s...  \n",
       "11  To reflect the broader context of augmented sh...  \n",
       "6   To reflect the expanded knowledge structure th...  \n",
       "12  The update incorporates detailed information a...  \n",
       "0   The TSPN represents a distinct specialized app...  \n",
       "1   To reflect the expanded knowledge structure th...  \n",
       "2   The new content enhances the existing concept ...  \n",
       "18  ColorFormer introduces a distinct approach to ...  \n",
       "22  SP-TAD represents a distinct approach to Tempo...  \n",
       "23  To reflect the inclusion of the child node tha...  \n",
       "3   The update reflects the inclusion of generativ...  \n",
       "19  To incorporate the newly added ColorFormer tec...  \n",
       "4   The update incorporates new insights into self...  \n",
       "24  The update incorporates enhanced clarity and d...  \n",
       "13  The new content enhances the existing concept ...  \n",
       "20  Char-Net represents a distinct approach within...  \n",
       "25  The new content introduces a distinct approach...  \n",
       "14  To reflect the broader application and techniq...  \n",
       "21  The update reflects the integration of charact...  \n",
       "26  To reflect the inclusion of Progressive Autore...  \n",
       "15  The update incorporates a more comprehensive u...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([t for t in tree.retrieve_all_logs() if isinstance(t,dict)])\n",
    "df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e78b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = NodeTree.load_from_file(\"results/single_to_single/gpt-4o-mini/gpt-4o-mini.snap.2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d66101",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = json.load(open(\"results/single_to_single/gpt-4o-mini/logs/gpt-4o-mini.logs.2.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "570a0b3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rich (Jupyter) Output:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000\">Advanced Computer Vision and Learning Techniques</span>\n",
       "├── <span style=\"color: #000000; text-decoration-color: #000000\">Self-Supervised Learning (SSL) Techniques[64,80]</span>\n",
       "│   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Contrastive Learning[11,30]</span>\n",
       "│   │   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Consistency-Based SSL[30]</span>\n",
       "│   │   └── <span style=\"color: #000000; text-decoration-color: #000000\">Contrastive Attention-Supervised Tuning (CAST)[69]</span>\n",
       "│   └── <span style=\"color: #000000; text-decoration-color: #000000\">Masked Image Modeling (MIM)[36]</span>\n",
       "│       ├── <span style=\"color: #000000; text-decoration-color: #000000\">Masked Feature Prediction (MaskFeat)[96]</span>\n",
       "│       └── <span style=\"color: #000000; text-decoration-color: #000000\">MixMIM Methodology[98]</span>\n",
       "├── <span style=\"color: #000000; text-decoration-color: #000000\">Object Detection Innovations[92]</span>\n",
       "│   ├── <span style=\"color: #000000; text-decoration-color: #000000\">End-to-End Object Detectors</span>\n",
       "│   │   ├── <span style=\"color: #000000; text-decoration-color: #000000\">DINO and Contrastive Denoising Training[40]</span>\n",
       "│   │   └── <span style=\"color: #000000; text-decoration-color: #000000\">Fitness NMS and Bounded IoU Loss[84]</span>\n",
       "│   └── <span style=\"color: #000000; text-decoration-color: #000000\">Multi-Dataset and Unified Detectors</span>\n",
       "│       ├── <span style=\"color: #000000; text-decoration-color: #000000\">Unified Multi-Dataset Detector[80]</span>\n",
       "│       └── <span style=\"color: #000000; text-decoration-color: #000000\">Object Detection and Classification Techniques[91]</span>\n",
       "│           └── <span style=\"color: #566c73; text-decoration-color: #566c73\">Video Relation Detection[746]</span>\n",
       "├── <span style=\"color: #000000; text-decoration-color: #000000\">Multi-Modal Representations and Learning[72]</span>\n",
       "│   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Vision-Language Models</span>\n",
       "│   │   ├── <span style=\"color: #000000; text-decoration-color: #000000\">NUWA Multimodal Pre-trained Model[52]</span>\n",
       "│   │   └── <span style=\"color: #000000; text-decoration-color: #000000\">Open-Vocabulary Semantic Segmentation[83]</span>\n",
       "│   └── <span style=\"color: #000000; text-decoration-color: #000000\">Universal Representations</span>\n",
       "│       ├── <span style=\"color: #000000; text-decoration-color: #000000\">Human Visual System's Consistency</span>\n",
       "│       └── <span style=\"color: #000000; text-decoration-color: #000000\">Machine Vision and Domain Generalization[72]</span>\n",
       "├── <span style=\"color: #000000; text-decoration-color: #000000\">Efficient and Scalable Network Architectures[68,77]</span>\n",
       "│   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Dynamic Token Sparsification</span>\n",
       "│   │   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Token Importance Scoring and Attention Masking</span>\n",
       "│   │   └── <span style=\"color: #000000; text-decoration-color: #000000\">Computational Efficiency Techniques[56]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Convolutional Neural Mixture Models[1724]</span>\n",
       "│   └── <span style=\"color: #000000; text-decoration-color: #000000\">Parameter-Free Operations</span>\n",
       "│       ├── <span style=\"color: #000000; text-decoration-color: #000000\">Hybrid Architectures</span>\n",
       "│       └── <span style=\"color: #000000; text-decoration-color: #000000\">Vision Transformers[68]</span>\n",
       "├── <span style=\"color: #000000; text-decoration-color: #000000\">3D Point Cloud Processing</span>\n",
       "│   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Self-Attention for Point Clouds</span>\n",
       "│   │   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Point Transformer Design[99]</span>\n",
       "│   │   └── <span style=\"color: #000000; text-decoration-color: #000000\">Position Encoding and Architecture</span>\n",
       "│   └── <span style=\"color: #000000; text-decoration-color: #000000\">Point-BERT for 3D Point Clouds</span>\n",
       "│       ├── <span style=\"color: #000000; text-decoration-color: #000000\">Point Cloud Tokenization</span>\n",
       "│       └── <span style=\"color: #000000; text-decoration-color: #000000\">Transfer Learning and Evaluation[92]</span>\n",
       "├── <span style=\"color: #000000; text-decoration-color: #000000\">Image Captioning and Scene Text Recognition</span>\n",
       "│   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Reinforcement Learning in Image Captioning</span>\n",
       "│   │   ├── <span style=\"color: #000000; text-decoration-color: #000000\">Self-Critical Sequence Training (SCST)[65]</span>\n",
       "│   │   └── <span style=\"color: #000000; text-decoration-color: #000000\">Non-Differentiable Metrics Optimization</span>\n",
       "│   └── <span style=\"color: #000000; text-decoration-color: #000000\">Attention-Based Scene Text Recognition</span>\n",
       "│       ├── <span style=\"color: #000000; text-decoration-color: #000000\">CNN Encoder and Decoder Design</span>\n",
       "│       └── <span style=\"color: #000000; text-decoration-color: #000000\">Simplified Training Data Requirements[95]</span>\n",
       "└── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Video Relation Detection[746]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;0;0;0mAdvanced Computer Vision and Learning Techniques\u001b[0m\n",
       "├── \u001b[38;2;0;0;0mSelf-Supervised Learning (SSL) Techniques[64,80]\u001b[0m\n",
       "│   ├── \u001b[38;2;0;0;0mContrastive Learning[11,30]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;0;0;0mConsistency-Based SSL[30]\u001b[0m\n",
       "│   │   └── \u001b[38;2;0;0;0mContrastive Attention-Supervised Tuning (CAST)[69]\u001b[0m\n",
       "│   └── \u001b[38;2;0;0;0mMasked Image Modeling (MIM)[36]\u001b[0m\n",
       "│       ├── \u001b[38;2;0;0;0mMasked Feature Prediction (MaskFeat)[96]\u001b[0m\n",
       "│       └── \u001b[38;2;0;0;0mMixMIM Methodology[98]\u001b[0m\n",
       "├── \u001b[38;2;0;0;0mObject Detection Innovations[92]\u001b[0m\n",
       "│   ├── \u001b[38;2;0;0;0mEnd-to-End Object Detectors\u001b[0m\n",
       "│   │   ├── \u001b[38;2;0;0;0mDINO and Contrastive Denoising Training[40]\u001b[0m\n",
       "│   │   └── \u001b[38;2;0;0;0mFitness NMS and Bounded IoU Loss[84]\u001b[0m\n",
       "│   └── \u001b[38;2;0;0;0mMulti-Dataset and Unified Detectors\u001b[0m\n",
       "│       ├── \u001b[38;2;0;0;0mUnified Multi-Dataset Detector[80]\u001b[0m\n",
       "│       └── \u001b[38;2;0;0;0mObject Detection and Classification Techniques[91]\u001b[0m\n",
       "│           └── \u001b[38;2;86;108;115mVideo Relation Detection[746]\u001b[0m\n",
       "├── \u001b[38;2;0;0;0mMulti-Modal Representations and Learning[72]\u001b[0m\n",
       "│   ├── \u001b[38;2;0;0;0mVision-Language Models\u001b[0m\n",
       "│   │   ├── \u001b[38;2;0;0;0mNUWA Multimodal Pre-trained Model[52]\u001b[0m\n",
       "│   │   └── \u001b[38;2;0;0;0mOpen-Vocabulary Semantic Segmentation[83]\u001b[0m\n",
       "│   └── \u001b[38;2;0;0;0mUniversal Representations\u001b[0m\n",
       "│       ├── \u001b[38;2;0;0;0mHuman Visual System's Consistency\u001b[0m\n",
       "│       └── \u001b[38;2;0;0;0mMachine Vision and Domain Generalization[72]\u001b[0m\n",
       "├── \u001b[38;2;0;0;0mEfficient and Scalable Network Architectures[68,77]\u001b[0m\n",
       "│   ├── \u001b[38;2;0;0;0mDynamic Token Sparsification\u001b[0m\n",
       "│   │   ├── \u001b[38;2;0;0;0mToken Importance Scoring and Attention Masking\u001b[0m\n",
       "│   │   └── \u001b[38;2;0;0;0mComputational Efficiency Techniques[56]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mConvolutional Neural Mixture Models[1724]\u001b[0m\n",
       "│   └── \u001b[38;2;0;0;0mParameter-Free Operations\u001b[0m\n",
       "│       ├── \u001b[38;2;0;0;0mHybrid Architectures\u001b[0m\n",
       "│       └── \u001b[38;2;0;0;0mVision Transformers[68]\u001b[0m\n",
       "├── \u001b[38;2;0;0;0m3D Point Cloud Processing\u001b[0m\n",
       "│   ├── \u001b[38;2;0;0;0mSelf-Attention for Point Clouds\u001b[0m\n",
       "│   │   ├── \u001b[38;2;0;0;0mPoint Transformer Design[99]\u001b[0m\n",
       "│   │   └── \u001b[38;2;0;0;0mPosition Encoding and Architecture\u001b[0m\n",
       "│   └── \u001b[38;2;0;0;0mPoint-BERT for 3D Point Clouds\u001b[0m\n",
       "│       ├── \u001b[38;2;0;0;0mPoint Cloud Tokenization\u001b[0m\n",
       "│       └── \u001b[38;2;0;0;0mTransfer Learning and Evaluation[92]\u001b[0m\n",
       "├── \u001b[38;2;0;0;0mImage Captioning and Scene Text Recognition\u001b[0m\n",
       "│   ├── \u001b[38;2;0;0;0mReinforcement Learning in Image Captioning\u001b[0m\n",
       "│   │   ├── \u001b[38;2;0;0;0mSelf-Critical Sequence Training (SCST)[65]\u001b[0m\n",
       "│   │   └── \u001b[38;2;0;0;0mNon-Differentiable Metrics Optimization\u001b[0m\n",
       "│   └── \u001b[38;2;0;0;0mAttention-Based Scene Text Recognition\u001b[0m\n",
       "│       ├── \u001b[38;2;0;0;0mCNN Encoder and Decoder Design\u001b[0m\n",
       "│       └── \u001b[38;2;0;0;0mSimplified Training Data Requirements[95]\u001b[0m\n",
       "└── \u001b[38;2;255;100;100mVideo Relation Detection[746]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print as rprint\n",
    "rich_output, term_output = tree.tree_snapshot_colored()\n",
    "print(\"Rich (Jupyter) Output:\")\n",
    "rprint(rich_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ba5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(tree, \"snap.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f5396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4258635c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content_metadata': {'content_id_address': 65}, 'is_used_for_update': True}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[\"Image Captioning and Scene Text Recognition\"][\"Reinforcement Learning in Image Captioning\"].represented_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1702ae0",
   "metadata": {
    "code_folding": [
     0,
     44
    ]
   },
   "outputs": [],
   "source": [
    "def export_to_json(self, indent: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Export the tree structure to a JSON string.\n",
    "\n",
    "    Args:\n",
    "        indent (int): Number of spaces for JSON indentation\n",
    "\n",
    "    Returns:\n",
    "        str: JSON string representation of the tree\n",
    "    \"\"\"\n",
    "    def _build_dict(node: NodeStructure,depth=0) -> Dict:\n",
    "        \"\"\"Helper method to build dictionary representation of the tree\"\"\"\n",
    "        # Base node attributes\n",
    "        represent_list = node.represented_pool\n",
    "        represents = [t['content_metadata']['content_id_address'] for t in represent_list]\n",
    "        node_dict = {\n",
    "            \"concept_key_word\": node.key,\n",
    "            \"concept_abstract\": node.concept_abstract,\n",
    "            \"concept_represents\":represents ,\n",
    "        }\n",
    "\n",
    "        # Handle children based on level\n",
    "        if node.children:\n",
    "            # Determine the sub-concept level\n",
    "            sub_concept_key = f\"sub_concept_{depth + 1}\"\n",
    "            node_dict[sub_concept_key] = [_build_dict(child,depth+1) for child in node.children]\n",
    "        else:\n",
    "            # Leaf nodes should have an empty sub_concept array\n",
    "            sub_concept_key = f\"sub_concept_{depth + 1}\"\n",
    "            node_dict[sub_concept_key] = []\n",
    "\n",
    "        return node_dict\n",
    "\n",
    "    # Build the full JSON structure\n",
    "    json_dict = {\n",
    "        \"title\": self.value.get(\"title\", \"Untitled\"),\n",
    "        \"area\": self.value.get(\"area\", \"\"),\n",
    "        \"idea_paradigm\": self.value.get(\"idea_paradigm\", \"\"),\n",
    "        \"root_level_concept\": [_build_dict(child,0) for child in self.children]\n",
    "    }\n",
    "\n",
    "    # Convert to JSON string with specified indentation\n",
    "    return json.dumps(json_dict, indent=indent)\n",
    "\n",
    "def save_to_json(self, filepath: str, indent: int = 4) -> None:\n",
    "    \"\"\"\n",
    "    Save the tree structure to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the output JSON file\n",
    "        indent (int): Number of spaces for JSON indentation\n",
    "    \"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(self.export_to_json(indent=indent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b626ef",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6003904a924dd8add7e841548c3481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Literature Research Report -> Efficient and Scalable Network Architectures -> Dynamic Token Sparsification -> Token Importance Scoring and Attention Masking -> New Child\n",
      "<- Action: ADD at Token Importance Scoring and Attention Mechanisms\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Consistency-Based SSL -> New Child\n",
      "<- Action: ADD at Consistency-Based Self-Supervised Learning\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors -> DINO and Contrastive Denoising Training\n",
      "<- Action: ADD at DINO and Object Detection Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Consistency-Based Self-Supervised Learning -> Nearest-Neighbor Contrastive Learning -> New Child\n",
      "<- Action: ADD at Nearest-Neighbor Contrastive Learning and Self-Supervised Learning\n",
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition -> Reinforcement Learning in Image Captioning -> New Child\n",
      "<- Action: ADD at Reinforcement Learning in Image Captioning and Object Relation Modeling\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> NUWA Multimodal Pre-trained Model -> New Child\n",
      "<- Action: ADD at Multimodal Pre-trained Models\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Contrastive Attention-Supervised Tuning (CAST) -> New Child\n",
      "<- Action: UPDATE at Contrastive Attention-Supervised Tuning (CAST)\n",
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors -> DINO and Object Detection Techniques -> Plain DETR Variants -> New Child\n",
      "<- Action: ADD at DETR Object Detection Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Unified Multi-Dataset Detector -> New Child\n",
      "<- Action: ADD at Unified Multi-Dataset Detection and Semantic Segmentation\n",
      "-> Literature Research Report -> Efficient and Scalable Network Architectures -> Parameter-Free Operations -> Hybrid Architectures -> New Child\n",
      "<- Action: ADD at Hybrid Architectures and Convolution Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Unified Multi-Dataset Detection and Semantic Segmentation -> Open-Vocabulary Semantic Segmentation -> New Child\n",
      "<- Action: ADD at Open-Vocabulary Semantic Segmentation and Universal Architectures\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Universal Representations -> Machine Vision and Domain Generalization -> New Child\n",
      "<- Action: ADD at Machine Vision and Compositional Learning\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors -> Fitness NMS and Bounded IoU Loss -> New Child\n",
      "<- Action: ADD at Object Detection Optimization\n",
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition -> Attention-Based Scene Text Recognition\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Universal Representations -> Machine Vision and Compositional Learning -> Compositional Zero-Shot Learning\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> New Child\n",
      "<- Action: ADD at Decoupling Detection, Classification, and Object-Centric Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> Masked Feature Prediction (MaskFeat) -> New Child\n",
      "<- Action: ADD at Masked Feature Prediction and Autoencoder Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> Multimodal Pre-trained Models -> Contrastive Captioners -> New Child\n",
      "<- Action: ADD at Vision-Language Models\n",
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition -> Attention-Based Scene Text Recognition\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM)\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Unified Multi-Dataset Detection and Semantic Segmentation -> Open-Vocabulary Semantic Segmentation and Universal Architectures -> Masked-attention Mask Transformer -> New Child\n",
      "<- Action: ADD at Masked-attention Mask Transformer and PACO Dataset\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors -> Object Detection Optimization -> Dynamic Anchor Shape Learning -> New Child\n",
      "<- Action: ADD at Dynamic Anchor Shape Learning and Scale-Transferrable Detection\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> Open-Vocabulary Semantic Segmentation -> New Child\n",
      "<- Action: ADD at Open-Vocabulary Semantic Segmentation and Region-Level Understanding\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> Multimodal Pre-trained Models -> Vision-Language Models -> InstructBLIP -> New Child\n",
      "<- Action: ADD at Vision-Language Models\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection, Classification, and Object-Centric Learning -> Unsupervised Object-Centric Learning -> New Child\n",
      "<- Action: ADD at Object-Centric Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> Masked Feature Prediction and Autoencoder Techniques -> Masked Autoencoder Contrastive Tuning (MAE-CT) -> New Child\n",
      "<- Action: ADD at Masked Autoencoder Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors\n",
      "<- Action: ADD at End-to-End Object Detection and Tracking\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Universal Representations -> Machine Vision and Compositional Learning -> Compositional Zero-Shot Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection and Tracking -> New Child\n",
      "<- Action: ADD at End-to-End Object Detection, Tracking, and Keypoint Detection\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection, Tracking, and Keypoint Detection -> New Child\n",
      "<- Action: ADD at End-to-End Object Detection, Tracking, Keypoint Detection, and Self-Supervised Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM)\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Consistency-Based Self-Supervised Learning -> Nearest-Neighbor Contrastive Learning and Self-Supervised Learning -> Part-Aware Self-Supervised Pre-Training -> New Child\n",
      "<- Action: ADD at Part-Aware Self-Supervised Learning\n",
      "-> Literature Research Report -> Efficient and Scalable Network Architectures -> Dynamic Token Sparsification -> Token Importance Scoring and Attention Mechanisms -> Focal Self-Attention -> New Child\n",
      "<- Action: ADD at Vision Transformers\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection, Tracking, Keypoint Detection, and Self-Supervised Learning\n",
      "-> Literature Research Report -> Object Detection Innovations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Consistency-Based Self-Supervised Learning -> Nearest-Neighbor Contrastive Learning and Self-Supervised Learning -> Part-Aware Self-Supervised Learning -> Conditional Augmentation-aware Self-supervised Learning -> New Child\n",
      "<- Action: ADD at Conditional Augmentation-aware Self-supervised Learning and Adaptive Spatial Aggregation\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection, Tracking, Keypoint Detection, and Self-Supervised Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Unified Multi-Dataset Detection and Semantic Segmentation -> Open-Vocabulary Semantic Segmentation and Universal Architectures -> Masked-attention Mask Transformer and PACO Dataset -> PACO -> New Child\n",
      "<- Action: UPDATE at PACO with MViT Integration\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> Masked Feature Prediction and Autoencoder Techniques -> Masked Autoencoder Techniques -> Semi-MAE -> New Child\n",
      "<- Action: UPDATE at Semi-MAE with TiTok\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> Open-Vocabulary Semantic Segmentation and Region-Level Understanding -> TAP Model -> New Child\n",
      "<- Action: ADD at TAP Model and Video Captioning Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection, Tracking, Keypoint Detection, and Self-Supervised Learning -> Object Detection Optimization -> Dynamic Anchor Shape Learning and Scale-Transferrable Detection\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> Masked Feature Prediction and Autoencoder Techniques -> Masked Autoencoder Techniques -> Semi-MAE with TiTok -> New Child\n",
      "<- Action: ADD at Semi-MAE and Attention-Guided Masking\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7ff2d843a050>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhangtianning.di/anaconda3/envs/camel2/lib/python3.10/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tree\u001b[38;5;241m.\u001b[39mbackpropagate()\n",
      "File \u001b[0;32m~/projects/TreeAgentKnowledge/tree_operations.py:312\u001b[0m, in \u001b[0;36mNodeTree.dispatch_content\u001b[0;34m(self, content, start_node)\u001b[0m\n\u001b[1;32m    309\u001b[0m path \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     decision:DispatchDecision \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dispatch_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m#print(decision)\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     path\u001b[38;5;241m.\u001b[39mappend(current_node\u001b[38;5;241m.\u001b[39mkey)\n",
      "File \u001b[0;32m~/projects/TreeAgentKnowledge/tree_operations.py:120\u001b[0m, in \u001b[0;36mNodeAgent.make_dispatch_decision\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make dispatch decision for content\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# message = AgentMessage(\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#         msg_type=MessageType.DISPATCH,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#         content=content,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#decision = self.generate_relevant_decision_via_simple_LLM(content)\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_relevant_decision_via_json_output_LLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# next_message=AgentMessage(\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#         msg_type=MessageType.DISPATCH,\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#         content={\"decision\": decision, \"original_content\": message.content},\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#         sender=self.key,\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#         receiver=message.sender\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decision\n",
      "File \u001b[0;32m~/projects/TreeAgentKnowledge/tree_operations.py:104\u001b[0m, in \u001b[0;36mNodeAgent.generate_relevant_decision_via_json_output_LLM\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mGenerate a decision about content relevance using LLM\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mThe simple means we have not hard structured the output in json format. Usually, structure json can be required by \u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m- `json_decoder` in sglang/outlines\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m- set `json_format` in openai api\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m user_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_relevant_decision_prompt(content)\n\u001b[0;32m--> 104\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(response\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;241m.\u001b[39mcontent) \u001b[38;5;66;03m## <-- make sure the response is a valid json\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbasement_polish\u001b[39m(string):\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/camel/agents/chat_agent.py:388\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, input_message, output_schema)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecord_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: BaseMessage) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Records the externally provided message into the agent memory as if\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    it were an answer of the :obj:`ChatAgent` from the backend. Currently,\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    the choice of the critic is submitted with this method.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m        message (BaseMessage): An external message to be recorded in the\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m            memory.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_memory(message, OpenAIBackendRole\u001b[38;5;241m.\u001b[39mASSISTANT)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/camel/agents/chat_agent.py:676\u001b[0m, in \u001b[0;36m_step_model_response\u001b[0;34m(self, openai_messages, num_tokens)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_token_exceed(\n\u001b[1;32m    671\u001b[0m         e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m], tool_call_records, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens_exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    672\u001b[0m     )\n\u001b[1;32m    674\u001b[0m (\n\u001b[1;32m    675\u001b[0m     response,\n\u001b[0;32m--> 676\u001b[0m     output_messages,\n\u001b[1;32m    677\u001b[0m     finish_reasons,\n\u001b[1;32m    678\u001b[0m     usage_dict,\n\u001b[1;32m    679\u001b[0m     response_id,\n\u001b[1;32m    680\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_model_response(openai_messages, num_tokens)\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tools_added()\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mtool_calls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    686\u001b[0m ):\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/camel/utils/commons.py:271\u001b[0m, in \u001b[0;36mapi_keys_required.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_api_key\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m missing_environment_keys\n\u001b[1;32m    267\u001b[0m ):\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing API keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_environment_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     )\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/camel/models/openai_model.py:112\u001b[0m, in \u001b[0;36mOpenAIModel.run\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 112\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/resources/chat/completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/_base_client.py:983\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    989\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http_proxy.py:344\u001b[0m, in \u001b[0;36mTunnelHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m HTTP11Connection(\n\u001b[1;32m    338\u001b[0m                 origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_origin,\n\u001b[1;32m    339\u001b[0m                 stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    340\u001b[0m                 keepalive_expiry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keepalive_expiry,\n\u001b[1;32m    341\u001b[0m             )\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/ssl.py:1258\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1256\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1257\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/ssl.py:1131\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,filename in enumerate(tqdm(os.listdir(Content_root))):\n",
    "    filepath = os.path.join(Content_root,filename )\n",
    "    with open(filepath,'r') as f:\n",
    "        content = f.read()\n",
    "    tree.dispatch_content(content)\n",
    "    tree.backpropagate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1356d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = tree.children_map['Self-Supervised Learning (SSL) Techniques'].children_map['Masked Image Modeling (MIM)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e405771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "prompt   = self.build_node_action_decision_prompt(content)\n",
    "response = self.step(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0345fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = eval(response.msg.content)\n",
    "details_models = {\"ADD\": AddActionDetails,\"MERGE\": MergeActionDetails}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932de2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_decision = details_models[response['action']](**response['details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8afc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddActionDetails(new_concept_key_word='ConMIM', new_concept_abstract='ConMIM is a novel masked image modeling approach that leverages contrastive learning for self-supervised pre-training of Vision Transformers, eliminating the need for pre-learned image tokenizers and introducing asymmetric designs to enhance denoising mechanisms.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956960eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95004e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self._execute_add_action(action_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee2e4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept Key: Masked Image Modeling (MIM)\n",
      "Concept Description: MIM focuses on predicting missing parts of the input data, often leading to improved understanding of the data structure and better feature extraction.\n",
      "Sub Concepts:\n",
      "    - [Masked Feature Prediction (MaskFeat)]:[Uses masked input sequences and predicts specific features such as HOG to enhance learning efficiency.]\n",
      "    - [MixMIM Methodology]:[A method that replaces MASK symbols with tokens from another image to improve training efficiency and generalization capabilities.]\n",
      "    - [ConMIM]:[ConMIM is a novel masked image modeling approach that leverages contrastive learning for self-supervised pre-training of Vision Transformers, eliminating the need for pre-learned image tokenizers and introducing asymmetric designs to enhance denoising mechanisms.]\n"
     ]
    }
   ],
   "source": [
    "print(self.text_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af951fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_node = self.children_map['ConMIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd379b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts.node_action.update_prompt_after_add_action import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a34d63f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f11b7393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept Key: Masked Image Modeling (MIM)\n",
      "Concept Description: MIM focuses on predicting missing parts of the input data, often leading to improved understanding of the data structure and better feature extraction.\n",
      "Sub Concepts:\n",
      "    - [Masked Feature Prediction (MaskFeat)]:[Uses masked input sequences and predicts specific features such as HOG to enhance learning efficiency.]\n",
      "    - [MixMIM Methodology]:[A method that replaces MASK symbols with tokens from another image to improve training efficiency and generalization capabilities.]\n",
      "    - [ConMIM]:[ConMIM is a novel masked image modeling approach that leverages contrastive learning for self-supervised pre-training of Vision Transformers, eliminating the need for pre-learned image tokenizers and introducing asymmetric designs to enhance denoising mechanisms.]\n"
     ]
    }
   ],
   "source": [
    "print(self.text_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48e8a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response=  self.step(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b224f664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_concept_abstract': 'Masked Image Modeling (MIM) focuses on predicting missing parts of the input data, often leading to improved understanding of the data structure and better feature extraction. This includes techniques such as Masked Feature Prediction and MixMIM Methodology, as well as the novel ConMIM approach, which leverages contrastive learning for self-supervised pre-training of Vision Transformers, enhancing denoising mechanisms and eliminating the need for pre-learned image tokenizers.',\n",
       " 'new_concept_key_word': 'Advanced Masked Image Modeling'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(response.msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2d86d0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision='LOWER' next_position='Self-Supervised Learning (SSL) Techniques' reasoning=\"The content specifically addresses advancements in self-supervised visual representation learning, particularly focusing on Vision Transformers and contrastive learning methods. This aligns closely with the child node 'Self-Supervised Learning (SSL) Techniques', making it suitable for placement there as it elaborates on specific techniques and innovations within that domain.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision='LOWER' next_position='Masked Image Modeling (MIM)' reasoning='The new content specifically addresses improvements in self-supervised visual representation learning using Vision Transformers, focusing on the effectiveness of contrastive learning in masked image modeling. This aligns directly with the sub-concept of Masked Image Modeling (MIM), making it suitable for placement under this child node.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision='LOWER' next_position='MixMIM Methodology' reasoning='The content discusses a novel method, ConMIM, which is a specific approach within the broader context of Masked Image Modeling (MIM). This method directly relates to the MixMIM Methodology as it proposes enhancements to the training efficiency and generalization capabilities of Vision Transformers, making it suitable for this child node.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision='LOWER' next_position='ConMIM Method' reasoning='The new content provides detailed insights into the ConMIM methodology, including its problem statement, key contributions, methods, and findings. This aligns closely with the specific focus of the MixMIM Methodology node, making it suitable for a child node dedicated to the ConMIM method.'\n",
      "=== create new node, then we need genereate the concept_key_word and concept_abstract ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Literature Research Report',\n",
       " 'Self-Supervised Learning (SSL) Techniques',\n",
       " 'Masked Image Modeling (MIM)',\n",
       " 'MixMIM Methodology',\n",
       " 'ConMIM Method']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.dispatch_content(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camel_good",
   "language": "python",
   "name": "camel_good"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
