{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da1706d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os,json\n",
    "base_url = \"https://api.gptsapi.net/v1\";model    = \"gpt-4o-mini\" \n",
    "# base_url = \"https://api.deepseek.com/beta\";model    = \"deepseek-chat\" \n",
    "with open(\".openai.conf.json\",'r') as f:\n",
    "    openai_config = json.load(f)\n",
    "api_key  = openai_config[base_url]['api_key'] #\n",
    "\n",
    "import os\n",
    "for var in \"http_proxy https_proxy HTTP_PROXY HTTPS_PROXY\".split():\n",
    "    os.environ[var]=\"\"\n",
    "\n",
    "base_url = \"http://10.140.24.65:30000/v1\"\n",
    "\n",
    "max_tokens= 16384\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']=api_key\n",
    "os.environ['OPENAI_API_BASE_URL']=base_url\n",
    "from tree_operations import *\n",
    "\n",
    "with open('extra_content.txt','r') as f:\n",
    "    content = f.read()\n",
    "tree = NodeTree.load_from_file(\"TreeKnowledge.example.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e82367",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = NodeTree.load_from_file(\"TreeKnowledge.example.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c68dbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m choose_pool \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m(i):k \u001b[38;5;28;01mfor\u001b[39;00m i,k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mchildren_keys)}\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@sgl\u001b[39m\u001b[38;5;241m.\u001b[39mfunction\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecide_next_position\u001b[39m(s):\n\u001b[1;32m      4\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sgl\u001b[38;5;241m.\u001b[39muser(system_prompt\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNow lets assume it is LOWER, please decide the next position for the content:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "choose_pool = {str(i):k for i,k in enumerate(self.children_keys)}\n",
    "@sgl.function\n",
    "def decide_next_position(s):\n",
    "    s += sgl.user(system_prompt+\"\\n\"+\"Now lets assume it is LOWER, please decide the next position for the content:\")\n",
    "    s += sgl.assistant(\n",
    "        sgl.gen(\n",
    "            \"answer\",\n",
    "            choices=list(choose_pool.keys()),\n",
    "            choices_method=sgl.greedy_token_selection,\n",
    "        )\n",
    "    )\n",
    "next_position = decide_next_position.run()['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b99fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01367ecd085497787ac8461617c11de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Content_root=\"custom_collection/graphrag/whole/summary_with_logic_position\"\n",
    "contents = []\n",
    "for i,filename in enumerate(tqdm(os.listdir(Content_root))):\n",
    "    filepath = os.path.join(Content_root,filename )\n",
    "    with open(filepath,'r') as f:\n",
    "        content = f.read()\n",
    "    contents.append(Content(i,content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497fbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.shuffle(contents)\n",
    "contents_split = np.array_split(contents,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c92a7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4c7f22610d4b1eb16dfd93333d987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> MixMIM Methodology -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> [None]\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> MixMIM Methodology\n",
      "<- Action: UPDATE at MixMIM Methodology\n",
      "<- Action: ADD at MixMIM Methodology\n",
      "<- Action: ADD at Image Captioning and Scene Text Recognition\n",
      "<- Action: UPDATE at End-to-End Object Detectors\n",
      "<- Action: ADD at End-to-End Object Detectors\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n",
      "<- Action: UPDATE at Decoupling Detection and Classification\n",
      "<- Action: UPDATE at Decoupling Detection and Classification\n",
      "<- Action: ADD at Decoupling Detection and Classification\n"
     ]
    }
   ],
   "source": [
    "for content_list in tqdm(contents_split[:1]):\n",
    "    already_passed_paper = set(tree.get_whole_represented_pool())\n",
    "    content_list = [content for content in content_list if int(content.id_address) not in tree.get_whole_represented_pool()]\n",
    "    await tree.dispatch_multiple_contents(list(content_list))\n",
    "    await tree.backpropagate(do_evolve=True)\n",
    "    tree.save_to_json(\"sglang.snap.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e0afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbb = tree.retrieve_all_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf81b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>old_concept_key_word</th>\n",
       "      <th>old_concept_abstract</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>action</th>\n",
       "      <th>new_concept_key_word</th>\n",
       "      <th>new_concept_abstract</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Image Captioning and Scene Text Recognition</td>\n",
       "      <td>Image Captioning and Scene Text Recognition</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-21-49-681198</td>\n",
       "      <td>add</td>\n",
       "      <td>Speculative Jacobi Decoding</td>\n",
       "      <td>Speculative Jacobi Decoding (SJD) is a novel, ...</td>\n",
       "      <td>SJD represents a distinct specialized method a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-21-50-159700</td>\n",
       "      <td>add</td>\n",
       "      <td>Augmented Shortcuts</td>\n",
       "      <td>Augmented Shortcuts are a novel method introdu...</td>\n",
       "      <td>Augmented shortcuts represent a distinct appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-21-50-178737</td>\n",
       "      <td>add</td>\n",
       "      <td>Scale-Steered Convolutional Networks</td>\n",
       "      <td>Scale-Steered Convolutional Networks (SS-CNNs)...</td>\n",
       "      <td>The introduction of SS-CNNs represents a disti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>Image Captioning and Scene Text Recognition</td>\n",
       "      <td>Recent approaches in image captioning and scen...</td>\n",
       "      <td>2024-11-02-16-21-51-113554</td>\n",
       "      <td>update</td>\n",
       "      <td>Recent approaches in image captioning and scen...</td>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>To better reflect the expanded knowledge struc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dynamic Token Optimization Techniques</td>\n",
       "      <td>Dynamic Token Sparsification</td>\n",
       "      <td>A framework that dynamically prunes tokens bas...</td>\n",
       "      <td>2024-11-02-16-21-54-301164</td>\n",
       "      <td>update</td>\n",
       "      <td>Dynamic Token Sparsification is a framework th...</td>\n",
       "      <td>Dynamic Token Optimization Techniques</td>\n",
       "      <td>To reflect the broader category of techniques ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Augmented Shortcuts</td>\n",
       "      <td>Augmented Shortcuts</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-22-466551</td>\n",
       "      <td>append</td>\n",
       "      <td>Paper 1869</td>\n",
       "      <td></td>\n",
       "      <td>The new content enhances the existing concept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vision-Language Models</td>\n",
       "      <td>Vision-Language Models</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-23-014679</td>\n",
       "      <td>add</td>\n",
       "      <td>MagicMix</td>\n",
       "      <td>MagicMix is a novel semantic mixing technique ...</td>\n",
       "      <td>MagicMix introduces a distinct technique for s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>Augmented Shortcuts</td>\n",
       "      <td>Augmented Shortcuts are a novel method introdu...</td>\n",
       "      <td>2024-11-02-16-22-23-806545</td>\n",
       "      <td>update</td>\n",
       "      <td>Augmented Shortcuts are a novel method introdu...</td>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>To reflect the broader context of augmented sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vision-Language Techniques</td>\n",
       "      <td>Vision-Language Models</td>\n",
       "      <td>Models that integrate visual and textual data ...</td>\n",
       "      <td>2024-11-02-16-22-24-598324</td>\n",
       "      <td>update</td>\n",
       "      <td>Vision-Language Models integrate visual and te...</td>\n",
       "      <td>Vision-Language Techniques</td>\n",
       "      <td>To reflect the expanded knowledge structure th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>Augmented Shortcuts are a novel method introdu...</td>\n",
       "      <td>2024-11-02-16-22-25-515673</td>\n",
       "      <td>update</td>\n",
       "      <td>Advanced neural network techniques, including ...</td>\n",
       "      <td>Advanced Neural Network Techniques</td>\n",
       "      <td>The update incorporates detailed information a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Literature Research Report</td>\n",
       "      <td>Literature Research Report</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-27-291699</td>\n",
       "      <td>add</td>\n",
       "      <td>Temporal Span Proposal Network</td>\n",
       "      <td>Temporal Span Proposal Network (TSPN) is a nov...</td>\n",
       "      <td>The TSPN represents a distinct specialized app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Advanced Learning and Detection Techniques</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-29-476492</td>\n",
       "      <td>update</td>\n",
       "      <td>Advancements in Self-Supervised Learning, Obje...</td>\n",
       "      <td>Advanced Learning and Detection Techniques</td>\n",
       "      <td>To reflect the expanded knowledge structure th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-Supervised Learning (SSL) Techniques</td>\n",
       "      <td>Self-Supervised Learning (SSL) Techniques</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-34-344804</td>\n",
       "      <td>append</td>\n",
       "      <td>Paper 1503</td>\n",
       "      <td></td>\n",
       "      <td>The new content enhances the existing concept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-34-460241</td>\n",
       "      <td>add</td>\n",
       "      <td>ColorFormer</td>\n",
       "      <td>ColorFormer is a transformer-based architectur...</td>\n",
       "      <td>ColorFormer introduces a distinct approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Temporal Span Proposal Network</td>\n",
       "      <td>Temporal Span Proposal Network</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-34-765169</td>\n",
       "      <td>add</td>\n",
       "      <td>Sparse Proposals with Hierarchical Features</td>\n",
       "      <td>Sparse Proposals with Hierarchical Features (S...</td>\n",
       "      <td>SP-TAD represents a distinct approach to Tempo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>Temporal Span Proposal Network</td>\n",
       "      <td>Temporal Span Proposal Network (TSPN) is a nov...</td>\n",
       "      <td>2024-11-02-16-22-36-597204</td>\n",
       "      <td>update</td>\n",
       "      <td>Temporal Span Proposal Network (TSPN) is a nov...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>To reflect the inclusion of the child node tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>Self-Supervised Learning (SSL) Techniques</td>\n",
       "      <td>Self-supervised learning has become a prominen...</td>\n",
       "      <td>2024-11-02-16-22-37-030887</td>\n",
       "      <td>update</td>\n",
       "      <td>Self-supervised learning has become a prominen...</td>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>The update reflects the inclusion of generativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>Recent approaches in image captioning and scen...</td>\n",
       "      <td>2024-11-02-16-22-37-625815</td>\n",
       "      <td>update</td>\n",
       "      <td>Recent approaches in image captioning and scen...</td>\n",
       "      <td>Image Captioning and Recognition Techniques</td>\n",
       "      <td>To incorporate the newly added ColorFormer tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>Self-supervised learning has become a prominen...</td>\n",
       "      <td>2024-11-02-16-22-39-195500</td>\n",
       "      <td>update</td>\n",
       "      <td>Self-supervised learning techniques are increa...</td>\n",
       "      <td>Self-Supervised Learning Techniques</td>\n",
       "      <td>The update incorporates new insights into self...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>Temporal Span Proposal Network (TSPN) is a nov...</td>\n",
       "      <td>2024-11-02-16-22-39-323019</td>\n",
       "      <td>update</td>\n",
       "      <td>The Temporal Span Proposal Network (TSPN) is a...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>The update incorporates enhanced clarity and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Point Transformer Design</td>\n",
       "      <td>Point Transformer Design</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-46-459857</td>\n",
       "      <td>append</td>\n",
       "      <td>Paper 1030</td>\n",
       "      <td></td>\n",
       "      <td>The new content enhances the existing concept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CNN Encoder and Decoder Design</td>\n",
       "      <td>CNN Encoder and Decoder Design</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-46-752473</td>\n",
       "      <td>add</td>\n",
       "      <td>Character-Aware Neural Network</td>\n",
       "      <td>Char-Net is a character-aware neural network d...</td>\n",
       "      <td>Char-Net represents a distinct approach within...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td></td>\n",
       "      <td>2024-11-02-16-22-46-933679</td>\n",
       "      <td>add</td>\n",
       "      <td>Progressive Autoregressive Video Diffusion Models</td>\n",
       "      <td>Progressive Autoregressive Video Diffusion Mod...</td>\n",
       "      <td>The new content introduces a distinct approach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>Point Transformer Design</td>\n",
       "      <td>Utilizes self-attention networks to process po...</td>\n",
       "      <td>2024-11-02-16-22-47-908857</td>\n",
       "      <td>update</td>\n",
       "      <td>Point Transformer Design utilizes self-attenti...</td>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>To reflect the broader application and techniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CNN Design for Text Recognition</td>\n",
       "      <td>CNN Encoder and Decoder Design</td>\n",
       "      <td>Uses a ResNet34-based CNN encoder and an atten...</td>\n",
       "      <td>2024-11-02-16-22-48-326479</td>\n",
       "      <td>update</td>\n",
       "      <td>CNN Encoder and Decoder Design utilizes a ResN...</td>\n",
       "      <td>CNN Design for Text Recognition</td>\n",
       "      <td>The update reflects the integration of charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Video Visual Relation Detection and Generation...</td>\n",
       "      <td>Temporal Span Proposal Network and Action Dete...</td>\n",
       "      <td>The Temporal Span Proposal Network (TSPN) is a...</td>\n",
       "      <td>2024-11-02-16-22-48-782653</td>\n",
       "      <td>update</td>\n",
       "      <td>The Temporal Span Proposal Network (TSPN) is a...</td>\n",
       "      <td>Video Visual Relation Detection and Generation...</td>\n",
       "      <td>To reflect the inclusion of Progressive Autore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>Point Transformer Design utilizes self-attenti...</td>\n",
       "      <td>2024-11-02-16-22-50-330780</td>\n",
       "      <td>update</td>\n",
       "      <td>Point cloud processing involves the use of sel...</td>\n",
       "      <td>Point Cloud Processing</td>\n",
       "      <td>The update incorporates a more comprehensive u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               parent  \\\n",
       "16        Image Captioning and Scene Text Recognition   \n",
       "7                        Dynamic Token Sparsification   \n",
       "8                        Dynamic Token Sparsification   \n",
       "17        Image Captioning and Recognition Techniques   \n",
       "9               Dynamic Token Optimization Techniques   \n",
       "10                                Augmented Shortcuts   \n",
       "5                              Vision-Language Models   \n",
       "11                 Advanced Neural Network Techniques   \n",
       "6                          Vision-Language Techniques   \n",
       "12                 Advanced Neural Network Techniques   \n",
       "0                          Literature Research Report   \n",
       "1          Advanced Learning and Detection Techniques   \n",
       "2           Self-Supervised Learning (SSL) Techniques   \n",
       "18        Image Captioning and Recognition Techniques   \n",
       "22                     Temporal Span Proposal Network   \n",
       "23  Temporal Span Proposal Network and Action Dete...   \n",
       "3                 Self-Supervised Learning Techniques   \n",
       "19        Image Captioning and Recognition Techniques   \n",
       "4                 Self-Supervised Learning Techniques   \n",
       "24  Temporal Span Proposal Network and Action Dete...   \n",
       "13                           Point Transformer Design   \n",
       "20                     CNN Encoder and Decoder Design   \n",
       "25  Temporal Span Proposal Network and Action Dete...   \n",
       "14                             Point Cloud Processing   \n",
       "21                    CNN Design for Text Recognition   \n",
       "26  Video Visual Relation Detection and Generation...   \n",
       "15                             Point Cloud Processing   \n",
       "\n",
       "                                 old_concept_key_word  \\\n",
       "16        Image Captioning and Scene Text Recognition   \n",
       "7                        Dynamic Token Sparsification   \n",
       "8                        Dynamic Token Sparsification   \n",
       "17        Image Captioning and Scene Text Recognition   \n",
       "9                        Dynamic Token Sparsification   \n",
       "10                                Augmented Shortcuts   \n",
       "5                              Vision-Language Models   \n",
       "11                                Augmented Shortcuts   \n",
       "6                              Vision-Language Models   \n",
       "12                 Advanced Neural Network Techniques   \n",
       "0                          Literature Research Report   \n",
       "1                                                       \n",
       "2           Self-Supervised Learning (SSL) Techniques   \n",
       "18        Image Captioning and Recognition Techniques   \n",
       "22                     Temporal Span Proposal Network   \n",
       "23                     Temporal Span Proposal Network   \n",
       "3           Self-Supervised Learning (SSL) Techniques   \n",
       "19        Image Captioning and Recognition Techniques   \n",
       "4                 Self-Supervised Learning Techniques   \n",
       "24  Temporal Span Proposal Network and Action Dete...   \n",
       "13                           Point Transformer Design   \n",
       "20                     CNN Encoder and Decoder Design   \n",
       "25  Temporal Span Proposal Network and Action Dete...   \n",
       "14                           Point Transformer Design   \n",
       "21                     CNN Encoder and Decoder Design   \n",
       "26  Temporal Span Proposal Network and Action Dete...   \n",
       "15                             Point Cloud Processing   \n",
       "\n",
       "                                 old_concept_abstract  \\\n",
       "16                                                      \n",
       "7                                                       \n",
       "8                                                       \n",
       "17  Recent approaches in image captioning and scen...   \n",
       "9   A framework that dynamically prunes tokens bas...   \n",
       "10                                                      \n",
       "5                                                       \n",
       "11  Augmented Shortcuts are a novel method introdu...   \n",
       "6   Models that integrate visual and textual data ...   \n",
       "12  Augmented Shortcuts are a novel method introdu...   \n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "18                                                      \n",
       "22                                                      \n",
       "23  Temporal Span Proposal Network (TSPN) is a nov...   \n",
       "3   Self-supervised learning has become a prominen...   \n",
       "19  Recent approaches in image captioning and scen...   \n",
       "4   Self-supervised learning has become a prominen...   \n",
       "24  Temporal Span Proposal Network (TSPN) is a nov...   \n",
       "13                                                      \n",
       "20                                                      \n",
       "25                                                      \n",
       "14  Utilizes self-attention networks to process po...   \n",
       "21  Uses a ResNet34-based CNN encoder and an atten...   \n",
       "26  The Temporal Span Proposal Network (TSPN) is a...   \n",
       "15  Point Transformer Design utilizes self-attenti...   \n",
       "\n",
       "                     timestamp  action  \\\n",
       "16  2024-11-02-16-21-49-681198     add   \n",
       "7   2024-11-02-16-21-50-159700     add   \n",
       "8   2024-11-02-16-21-50-178737     add   \n",
       "17  2024-11-02-16-21-51-113554  update   \n",
       "9   2024-11-02-16-21-54-301164  update   \n",
       "10  2024-11-02-16-22-22-466551  append   \n",
       "5   2024-11-02-16-22-23-014679     add   \n",
       "11  2024-11-02-16-22-23-806545  update   \n",
       "6   2024-11-02-16-22-24-598324  update   \n",
       "12  2024-11-02-16-22-25-515673  update   \n",
       "0   2024-11-02-16-22-27-291699     add   \n",
       "1   2024-11-02-16-22-29-476492  update   \n",
       "2   2024-11-02-16-22-34-344804  append   \n",
       "18  2024-11-02-16-22-34-460241     add   \n",
       "22  2024-11-02-16-22-34-765169     add   \n",
       "23  2024-11-02-16-22-36-597204  update   \n",
       "3   2024-11-02-16-22-37-030887  update   \n",
       "19  2024-11-02-16-22-37-625815  update   \n",
       "4   2024-11-02-16-22-39-195500  update   \n",
       "24  2024-11-02-16-22-39-323019  update   \n",
       "13  2024-11-02-16-22-46-459857  append   \n",
       "20  2024-11-02-16-22-46-752473     add   \n",
       "25  2024-11-02-16-22-46-933679     add   \n",
       "14  2024-11-02-16-22-47-908857  update   \n",
       "21  2024-11-02-16-22-48-326479  update   \n",
       "26  2024-11-02-16-22-48-782653  update   \n",
       "15  2024-11-02-16-22-50-330780  update   \n",
       "\n",
       "                                 new_concept_key_word  \\\n",
       "16                        Speculative Jacobi Decoding   \n",
       "7                                 Augmented Shortcuts   \n",
       "8                Scale-Steered Convolutional Networks   \n",
       "17  Recent approaches in image captioning and scen...   \n",
       "9   Dynamic Token Sparsification is a framework th...   \n",
       "10                                         Paper 1869   \n",
       "5                                            MagicMix   \n",
       "11  Augmented Shortcuts are a novel method introdu...   \n",
       "6   Vision-Language Models integrate visual and te...   \n",
       "12  Advanced neural network techniques, including ...   \n",
       "0                      Temporal Span Proposal Network   \n",
       "1   Advancements in Self-Supervised Learning, Obje...   \n",
       "2                                          Paper 1503   \n",
       "18                                        ColorFormer   \n",
       "22        Sparse Proposals with Hierarchical Features   \n",
       "23  Temporal Span Proposal Network (TSPN) is a nov...   \n",
       "3   Self-supervised learning has become a prominen...   \n",
       "19  Recent approaches in image captioning and scen...   \n",
       "4   Self-supervised learning techniques are increa...   \n",
       "24  The Temporal Span Proposal Network (TSPN) is a...   \n",
       "13                                         Paper 1030   \n",
       "20                     Character-Aware Neural Network   \n",
       "25  Progressive Autoregressive Video Diffusion Models   \n",
       "14  Point Transformer Design utilizes self-attenti...   \n",
       "21  CNN Encoder and Decoder Design utilizes a ResN...   \n",
       "26  The Temporal Span Proposal Network (TSPN) is a...   \n",
       "15  Point cloud processing involves the use of sel...   \n",
       "\n",
       "                                 new_concept_abstract  \\\n",
       "16  Speculative Jacobi Decoding (SJD) is a novel, ...   \n",
       "7   Augmented Shortcuts are a novel method introdu...   \n",
       "8   Scale-Steered Convolutional Networks (SS-CNNs)...   \n",
       "17        Image Captioning and Recognition Techniques   \n",
       "9               Dynamic Token Optimization Techniques   \n",
       "10                                                      \n",
       "5   MagicMix is a novel semantic mixing technique ...   \n",
       "11                 Advanced Neural Network Techniques   \n",
       "6                          Vision-Language Techniques   \n",
       "12                 Advanced Neural Network Techniques   \n",
       "0   Temporal Span Proposal Network (TSPN) is a nov...   \n",
       "1          Advanced Learning and Detection Techniques   \n",
       "2                                                       \n",
       "18  ColorFormer is a transformer-based architectur...   \n",
       "22  Sparse Proposals with Hierarchical Features (S...   \n",
       "23  Temporal Span Proposal Network and Action Dete...   \n",
       "3                 Self-Supervised Learning Techniques   \n",
       "19        Image Captioning and Recognition Techniques   \n",
       "4                 Self-Supervised Learning Techniques   \n",
       "24  Temporal Span Proposal Network and Action Dete...   \n",
       "13                                                      \n",
       "20  Char-Net is a character-aware neural network d...   \n",
       "25  Progressive Autoregressive Video Diffusion Mod...   \n",
       "14                             Point Cloud Processing   \n",
       "21                    CNN Design for Text Recognition   \n",
       "26  Video Visual Relation Detection and Generation...   \n",
       "15                             Point Cloud Processing   \n",
       "\n",
       "                                               reason  \n",
       "16  SJD represents a distinct specialized method a...  \n",
       "7   Augmented shortcuts represent a distinct appro...  \n",
       "8   The introduction of SS-CNNs represents a disti...  \n",
       "17  To better reflect the expanded knowledge struc...  \n",
       "9   To reflect the broader category of techniques ...  \n",
       "10  The new content enhances the existing concept ...  \n",
       "5   MagicMix introduces a distinct technique for s...  \n",
       "11  To reflect the broader context of augmented sh...  \n",
       "6   To reflect the expanded knowledge structure th...  \n",
       "12  The update incorporates detailed information a...  \n",
       "0   The TSPN represents a distinct specialized app...  \n",
       "1   To reflect the expanded knowledge structure th...  \n",
       "2   The new content enhances the existing concept ...  \n",
       "18  ColorFormer introduces a distinct approach to ...  \n",
       "22  SP-TAD represents a distinct approach to Tempo...  \n",
       "23  To reflect the inclusion of the child node tha...  \n",
       "3   The update reflects the inclusion of generativ...  \n",
       "19  To incorporate the newly added ColorFormer tec...  \n",
       "4   The update incorporates new insights into self...  \n",
       "24  The update incorporates enhanced clarity and d...  \n",
       "13  The new content enhances the existing concept ...  \n",
       "20  Char-Net represents a distinct approach within...  \n",
       "25  The new content introduces a distinct approach...  \n",
       "14  To reflect the broader application and techniq...  \n",
       "21  The update reflects the integration of charact...  \n",
       "26  To reflect the inclusion of Progressive Autore...  \n",
       "15  The update incorporates a more comprehensive u...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([t for t in tree.retrieve_all_logs() if isinstance(t,dict)])\n",
    "df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e78b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = NodeTree.load_from_file(\"results/single_to_single/gpt-4o-mini/gpt-4o-mini.snap.23.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d66101",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = json.load(open(\"results/single_to_single/gpt-4o-mini/logs/gpt-4o-mini.logs.2.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570a0b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rich (Jupyter) Output:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff6464; text-decoration-color: #ff6464\">Literature Research Report</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Learning and Vision Applications[64,80]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Contrastive Learning Techniques and Minimal Augmentation[11,30]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Consistency in Visual Recognition[30,1686]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visformer[2066]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Attention-Based Learning Techniques[69]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Neural Network Architectures and Self-Supervised Learning[1846,463]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Cross-Batch Memory[1471]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Learning Techniques[260,1083]</span>\n",
       "│   │   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Contrastive Attention and Visual Representation Techniques[1679,489]</span>\n",
       "│   │   │               └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SLIP[1510]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Detection and Classification Techniques[1288,978,1696]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Lifted Contrastive Learning and Instance Localization[376,1132,1220]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Learning in Computer Vision[459,245,452]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">View-Agnostic Dense Representation Learning[115,389,1548]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">ContrastiveCrop[1960]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Minimal Augmentation Techniques[1260]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Masked Image Modeling Techniques and Scalable Learning[36,605,774]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Masked Feature Learning[96]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Learning Frameworks[763]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Visual Learning[1924]</span>\n",
       "│   │   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Masked Image Residual Learning[1010]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MixMIM Methodology and Masked Autoencoder Contrastive Tuning[98]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Masked Autoencoding Techniques[1131,501]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Semantic-Guided Masked Autoencoders and Generative Modeling[1726,1660,1873]</span>\n",
       "│   │   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MAAGE Framework and Its Components[806,1598]</span>\n",
       "│   │   │               ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">ConvNeXt V2 and Mixed Autoencoders[2160]</span>\n",
       "│   │   │               │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Visual Representation Learning[1059,677,1200,1265]</span>\n",
       "│   │   │               │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">DeepMIM and Pre-Pretraining Techniques[1536]</span>\n",
       "│   │   │               │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Pre-Pretraining with MAE[798]</span>\n",
       "│   │   │               └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SkeletonMAE[1035]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Data Scaling in MIM[1835]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Generative Hierarchical Feature[1066,74]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unsupervised Domain Generalization[2004,41]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Enhanced Denoising Techniques[1313,1511]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Knowledge Distillation Techniques[1733]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Replace one Branch[1432]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Extreme-Multi-Patch Self-Supervised Learning[571]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Attention Mechanisms in Vision Models[582,893,2028]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Attention Free Transformer[1039]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision GNN[330]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Video Object Segmentation and MultiSiam[801,1666,1850]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MultiSiam[2135]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">DEtection TRansformer[2154]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dense Teacher[1524]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Guided Diffusion Models[1456]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Scene Text Detection and Recognition[1618]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Information-Theoretic Analysis of Self-Supervised Learning[2026]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Visual Recognition and Semi-Supervised Learning Techniques[92,1221]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Object Detection and Knowledge Distillation Techniques</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Object Detection Techniques[40,378]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">NamedMask[1649]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Object Detection and Adaptivity Innovations[936]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dimension-Decomposition Region Proposal Network (DeRPN)[855]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Object Detection and Localization Techniques[84]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Object Detection Models[165,2151]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SOLO[407]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Weakly Supervised Learning Techniques and Object Detection[1326,555,1669]</span>\n",
       "│   │   │       │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Neural Network Techniques[446,400]</span>\n",
       "│   │   │       │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Enhanced Object Detection Techniques[1884,478]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SqueezeDet[1576]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Ambiguity-Resistant Semi-supervised Learning[1740]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">YOsO Framework[697]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Deep Snake[1204]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Montage Pre-training[1748]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Curve Text Detection[736]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Focal-Stable-DINO[1167]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">CoordConv[1887]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Feature Extraction in Computer Vision[904,667]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Object Relation and Representation Learning Techniques[1693]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Region-to-Object Representation Learning and YOLOS[898]</span>\n",
       "│   │   │       │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">YOLOS[1015]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Anti-Aliasing in Downsampling Layers[89]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Neural Network Techniques[1752]</span>\n",
       "│   │   │           ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Robust Vision Transformer[311]</span>\n",
       "│   │   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Selective Feature Connection Mechanism[1790]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Guided Anchoring[545]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Conditional DETR[822,1417,1380]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Bi-Level Optimized Query Slot Attention[21]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">TextScanner[1196]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Scale Normalization for Image Pyramids (SNIP)[819]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">DRAEM[1615]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">JoinDet[405]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Tokens-to-Token Vision Transformers[1659]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">RepFusion[1522]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">FCOS[1837]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Fast Knowledge Distillation[2068]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unified Detection Methods</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unified Multi-Dataset Detector[80]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Object Detection Frameworks[91]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">STAC Framework and ViLD[916]</span>\n",
       "│   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">ViLD[299]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Detic[1923]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Grounded Language-Image Pre-training[785]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Multi-Label Image Recognition[167]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Pix2Seq Framework[1719]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Deep Seeded Region Growing[896]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Open-World Instance Segmentation[1480]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Zero-Shot Semantic Segmentation[1436]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unified Panoptic and Open-World Segmentation[768,1657,207]</span>\n",
       "│   │           ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Instance Segmentation Frameworks[1363]</span>\n",
       "│   │           │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unified Segmentation Models[1335]</span>\n",
       "│   │           │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unsupervised Learning Models[1970]</span>\n",
       "│   │           │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Hybrid Task Cascade[914]</span>\n",
       "│   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">CAT: Cascade Detection Transformer[1348]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Query-Efficient Meta Attacks[767]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Open-Set Face Synthesis and Image Segmentation[987,593]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Image Segmentation Models[723]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SegGPT[231]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MaX-DeepLab[1552]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Pyramid Fusion Transformer and Cross Pseudo Supervision[1025]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Cross Pseudo Supervision (CPS)[166,1199]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual Relationship Detection Frameworks[2064]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SEMPART[594]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">ConMIM in Visual Task Applications[687,771]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Contextual Semantic Segmentation Techniques[946,1180]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SimFIR[332]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Deja Vu Memorization[279]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Attention Mechanisms[1753,2020,1427,541,865]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">RepMLP[310]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Instance Discrimination with Multi-crop and CutMix[1912]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Open-Vocabulary Object Detection[32,217,1614]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Zero-shot Unsupervised Transfer Instance Segmentation[2111]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Object Detection[935]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Open-Vocabulary Object Detection[284]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Conditional Variational Autoencoder (CVAE) for Feature Generation[631]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Deformable DETR[235]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MixupE[1237]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Integral Migration of Pre-trained Vision Transformers[662]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Attention Clusters[1795]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Zero-Shot Visual Recognition[1796]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Attention-Based Fine-Grained Transfer Learning[1859]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Text Detection Techniques[1971,1489]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Text Detection and Recognition[105]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SATRN[650]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MetNet-2[1347]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Attention Mechanisms in NLP[1234]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">FoveaBox[1560]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Implicit Slot Attention[1783]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Generative Models vs. Contrastive Learning in Few-Shot Segmentation[913]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Semantic-SAM[1521]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">DeepLIFT[1540]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Macaron Net[1324]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SemCo[908]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Multi-Modal Learning and Hypernetworks[72]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision-Language and Instructional Models</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Multimodal Learning and Optimization Models[52]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision-and-Language and Multimodal Models[31]</span>\n",
       "│   │   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Video Models and Human-Object Interaction[617]</span>\n",
       "│   │   │   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Receptive Field Block in CNN Architectures[1845,1355]</span>\n",
       "│   │   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Human-Object Interaction Detection[1045]</span>\n",
       "│   │   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Attention Mechanisms and Language Modeling[1320]</span>\n",
       "│   │   │   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dynamic Capsule Attention[960,1590]</span>\n",
       "│   │   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision-Language Pretraining Techniques and VL-BERT[1233,227]</span>\n",
       "│   │   │   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">VL-BERT[700]</span>\n",
       "│   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Gemini Multimodal Models[2040]</span>\n",
       "│   │   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">VisionLLM[1005]</span>\n",
       "│   │   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Multimodal Language-Image Models[572]</span>\n",
       "│   │   │   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Multimodal AI[2052,291]</span>\n",
       "│   │   │   │               └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Contrastive Captioners[238]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Context Optimization Techniques[2089,1520,2058]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Open-Vocabulary Semantic Segmentation[83]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unsupervised Text-to-Image Generation[1799]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual Prompting[1652]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Instruct Diffusion[216]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Universal Representations</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Human Visual System's Consistency</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Machine Vision and Multimodal Learning[72]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Meta-Transformer[761]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Temporal Span Proposal Network[342]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Adversarial Multi-Task Learning Techniques[2021]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unified Image Understanding[130]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Detection Hub[1172]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Synthetic Data-driven Learning Techniques[540]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Multi-Task Self-Training[1717]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Interpretable Convolutional Neural Networks[928]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Hypernetworks[387]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Network Architectures and Visual Representation Techniques[68,77]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Network Optimization Techniques and Efficient Transformers</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Attention Mechanisms and Global Context in Vision Transformers</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision Transformer Architectures and Attention Mechanisms[2046,859,1586,1515]</span>\n",
       "│   │   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Vision Transformers[743,1701,94,0]</span>\n",
       "│   │   │   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual Transformers[1920]</span>\n",
       "│   │   │   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">DeepViT[781]</span>\n",
       "│   │   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">So-ViT[1843]</span>\n",
       "│   │   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Pyramid Vision Transformer v2[1475]</span>\n",
       "│   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Attention Mechanisms in Vision Transformers[2103]</span>\n",
       "│   │   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Conditional Positional Encodings[1276]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Pale-Shaped Self-Attention and Semi-Supervised Learning[379,1759]</span>\n",
       "│   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Semi-MAE[123]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision Transformer Architectures[1033]</span>\n",
       "│   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision Transformer Architectures[625]</span>\n",
       "│   │   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">ViT-Slim[1621]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Extended Transformer Construction[919]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Enhanced Local Self-Attention[430]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Active Memory Models[1068]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dual PatchNorm[286]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Global Context Networks[445]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Computational Efficiency in Deep Learning[56]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Efficient Deep Learning Techniques[1472,23]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Turbo Training and Efficiency Optimization[1371]</span>\n",
       "│   │   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">BlockDrop[664]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Network Design Methodologies[1365,765]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">UniNet[791]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">UniNet[1688]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Generative Models[170]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Fixed Point Diffusion Models and MLP Architectures[1940]</span>\n",
       "│   │   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MLP-Mixer[1746]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Bayesian SegNet[872]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Binary Spherical Quantization[178]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Macaron Net Architecture[1236,202]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Adaptive Graph Convolutional Networks[635]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Matryoshka Representation Learning[2142]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Efficient Transformers[735]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Parameter-Free Operations</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Neural Network Architectures</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Projection and Rejection Products in Neural Networks[751,639]</span>\n",
       "│   │   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Graph HyperNetworks[1781]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Transformer Set Prediction Network[412]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision Transformers[68,1310]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Visual Representation and Semantic Recognition Techniques[146,1650]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Flow Capsules and Vision Transformers[1528]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Inception Transformer[433]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Foveated Vision and Bayesian Segmentation Models[1609]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Bayesian SegNet[1029]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Transformers in Visual Recognition[2013]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Multiscale Vision Transformers[204,1168]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual Autoregressive Modeling[973]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Adaptive Embedding Gate[1965]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Semantic Sparsity in Situation Recognition[1082]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Randomly Wired Neural Networks and Advanced Architectures[1488]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Cutting-edge Neural Network Architectures[1440]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">CORnet-S and Vision Computing[1333]</span>\n",
       "│   │       │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision Reservoir Computing[396]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Graph Neural Network Architectures[449,902]</span>\n",
       "│   │       │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">GLOM[653]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">X-volution[1282]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Nested Transformers[180]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">CMT and Transferable Architecture Learning[654]</span>\n",
       "│   │       │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Transferable Architecture Learning[329]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Perceiver IO[1322]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">ReNet[1298]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Temporal Value Modeling and Key Aggregation[1404]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Sparse Manifold Transform[195]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Coordinated Policy Optimization[298]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Learning Techniques[264,878]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Computational Prototype Model[183]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">FNet[256]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Graphormer[1372]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual Attention Model for Clothing Retrieval[931]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Invariance Learning in Vision Transformers[925]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">3D Visual Processing and Human Pose Estimation</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Attention for Point Clouds</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Point Cloud Processing Techniques and Frameworks[99,1663]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Implicit Autoencoder Framework and Masked Autoencoders[479,117]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Masked Autoencoders for Point Clouds[1289]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Position Encoding and Architecture</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Point-BERT for 3D Point Clouds</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Point Cloud Processing and Recognition Techniques[1384]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">3D Point Cloud Recognition Techniques[346,209]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Transfer Learning and Evaluation[92]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dynamic Graph CNN[1860]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Mixture Model CNNs[578]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">BlendedMVS Dataset[2119]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unsupervised Image Segmentation Techniques and Frameworks[1988,1158]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">PointContrast[1014]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">InterHand2.6M Dataset[1454]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Diffusion Model for Dense Visual Prediction[1578]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">3D Concept Learning and Reasoning Framework[174]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">RIM-Net[811]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Situation Recognition and Grounded Situation Recognition[249]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Grounded Situation Recognition[1448]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced 3D Object Detection Frameworks[297,1446]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dense Human Pose Estimation[758]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Image Processing and Computer Vision Techniques[783]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Reinforcement Learning in Image Captioning and Optimization[1544]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Reinforcement Learning Techniques[65,556,844]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Image Captioning Techniques[1382,823]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Actor-Critic Sequence Training[1061]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Image Captioning Metrics Optimization</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Diversity Metrics[1551]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Adaptive Image Captioning and Neural Frameworks[1354,883]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Neural Baby Talk[717]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Scene Recognition Techniques[155]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">CNN Text Processing and Transformation Networks</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Geometry-aware Text Recognition and Augmentation Techniques[933]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Arabic Scene Text Recognition[741]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Fast Oriented Text Spotting[853]</span>\n",
       "│   │   │       │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Neural Architectures for Text Recognition[882,1961,623,696,331,78,2032]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">STEFANN[25]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MASTER[655]</span>\n",
       "│   │   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Learnable Data Augmentation[1169]</span>\n",
       "│   │   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Manifold Mixup[341]</span>\n",
       "│   │   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Efficient Data Preparation Strategies[95,1089]</span>\n",
       "│   │   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Edit Probability in Text Recognition[1880,1772]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SphereFace[1046]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">DragonDiffusion[1980]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">FreeDrag[1736]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Density-Aware Image Restoration[2039]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Image Restoration Techniques[251]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Adjacent Aggregation Network[676]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Hierarchical Integration Diffusion Model[938]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Normalizer-Free ResNets and Harmonic Networks[124]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Harmonic Networks[443]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual Attention and Interpretability Techniques[62]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Grad-CAM[949]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unsupervised Semantic Segmentation Techniques[1146]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Segmentation Techniques[118,1078]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Semantic Image Segmentation Framework[619,665]</span>\n",
       "│   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dense Learning Frameworks[621]</span>\n",
       "│   │               ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Soft Proposal Networks and Related Methodologies[364,1767]</span>\n",
       "│   │               └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">End-to-End Instance Segmentation with Recurrent Attention[1969]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">High-Resolution Image Synthesis Techniques[144,1353]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Latent Diffusion Models and Applications[862]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Image Synthesis and Editing Frameworks[129]</span>\n",
       "│   │           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">High-Resolution Image Generation and Neural Network Architectures[1444]</span>\n",
       "│   │               ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">High-Resolution Image Synthesis and Virtual Try-On[368]</span>\n",
       "│   │               │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">VITON[1318]</span>\n",
       "│   │               └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Mobile-Former and Visual AutoRegressive Modeling[1852]</span>\n",
       "│   │                   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual AutoRegressive Modeling[1587]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Long Short-Term Memory (LSTM) in Sequence Generation[1858]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Generative Object Compositing Techniques[1947,1992]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dynamic Least-to-Most Prompting[440]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised Vision Transformers[1596]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Visual Transformers[918]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dynamic Vision Transformer[542]</span>\n",
       "│   │       ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">VOLO (Vision Outlooker)[728]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Masked Autoencoders[810]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual Recognition by Request[1296]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Robust Dense Feature Matching[897]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Continual Detection Transformer[1705]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Enhanced Simple Online and Realtime Tracking with Deep Association Metric[35]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Scale Steerable Filters[1129]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Supervised In-Context Learning (SINC)[809]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Position Information in CNNs and ViTs with Applications in Depth Estimation[1285,2147,224,747]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Cascaded ConvNet for Human Pose Estimation[691]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Gaussian Attention Bias[468]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Monocular Depth Estimation[1854]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Weather Forecasting Technologies[1208,472]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Polynomial Activation Functions[1457,1139,769,557]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">YOLOv7[1678]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Nonlocal Echo Dynamics Network[295]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Vision Transformers and Specialized Architectures[577]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Motion-Guided Global-Local Aggregation Transformer Network[846]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Score-Based Diffusion Models for Weather Forecasting[126]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Variational Inference Techniques[975,127]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Differentiable Search Index[1097]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Interpretable and Unified Machine Learning Approaches[1585,2000,1359,1452]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">CNN Interpretability and Object Detection Frameworks[306,705]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dynamic Head Framework and Interpretability Techniques[703]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Grad-CAM[2129]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Attribution Priors and Weakly-Supervised Techniques[1730]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Weakly-Supervised Learning Techniques[1049,1760]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Weakly Supervised Pre-training with Noisy Labels[1327]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Optical Flow Guided Feature[531]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Nowcasting Models[690,1114]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Modified LSTM Architectures for Improved Interpretability[2096]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SmoothGrad[1007]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unified Modeling Approaches[790]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Face Clustering on Affinity Graphs[2150]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Temporal Action Detection and Human-Object Interaction Frameworks[1115,47]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Parallel Point Detection and Matching[2045]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">AnyDoor[1793]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Spatial-Temporal Baseline for VideoHOI[721]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Monolingual Corpora Integration[1816]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">CoNe[363]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Video Understanding and Motion Estimation Technologies[598,305]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Lexicon-Free OCR[1635]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Video Visual Relation Detection and Reasoning[627]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Visual Transformation and Segmentation Techniques[704]</span>\n",
       "│   │       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Mask-Free Video Instance Segmentation[2014]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Non-Contrastive Self-Supervised Learning and Efficient CNN Inference[1831]</span>\n",
       "│   │   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Recurrent Residual Module[567]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">OmniMotion[1875]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Task Transfer Across Domains[766]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Benchmarking Graph Neural Networks[1085]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Benchmarking Frameworks for GNNs[1092]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">HyperPrompt[1825]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Feature Denoising[808]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Knowledge Factorization and Dynamic Prior Knowledge[1219]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Dynamic Prior Knowledge[254]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Simplex Autoencoders[684]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Few-Shot Learning Techniques[1070]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Annotator Learning with GANs[1476]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SANet[1430]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Auto-Reconstructor Network (ARNet)[1013]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Few-Shot Learning in Image Classification[1319]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Hardness-Aware Deep Metric Learning[236]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Hardness-Aware Deep Metric Learning[1081]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Test-Time Batch Normalization[870]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">AI Academic Competitiveness[201]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">AI Academic Resource Strategies[1482]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Deep Learning Techniques[695]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">LookupViT and RepMLP[911]</span>\n",
       "│       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Image Recognition and Generation Techniques[1620]</span>\n",
       "│           └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Diffusion Transformers[1368]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Sequence Modeling Techniques[1770]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Sequence Modeling Techniques[12]</span>\n",
       "│       └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Self-Critical Sequence Training[744]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Advanced Semantic Segmentation Techniques[554]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Cold Fusion[87]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">ViT-Adapter[461]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MaskFormer[793]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">GD-CAF Model[1283]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Human vs. Machine Attention in Image Captioning[2122]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">SimMIM[756]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Open-Vocabulary Semantic Segmentation and Adaptive Techniques[709,361]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unsupervised Semantic Segmentation[1738]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Label Mask AutoEncoder[1804]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Common Object Counting[1241]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Nonparametric Semantic Segmentation[502]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Hierarchical Vision Transformers and Object Detection Techniques[730,860,1927,2124]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">AttentionMask[776]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Arbitrary-Oriented Text Detection[1564]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">MixPro[2084]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Skip-Attention (SkIPAT)[481]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Efficient Self-Attention Mechanisms and Architectures in Vision and Speech Recognition[1683]</span>\n",
       "│   ├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Efficient Transformer Architectures[689,13]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Joint CTC-Attention Mechanism[59]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Skip-Attention (SkIPAT) for Reduced Computational Complexity[2038]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Skip-Attention[1135]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Instance Segmentation and Simulation[1518]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">CARLA Simulator[359]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Unsupervised Learning in Image Segmentation[203,1420,1410]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Extremal Perturbations[380]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Extremal Perturbations[983]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Exemplar Compression in Incremental Learning[795,1757]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Sparsifiner[1640]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Rotation-Sensitive Text Detection and k-NN Attention[210]</span>\n",
       "│   └── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">k-NN Attention[390]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Sparsifiner[1840]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Sparse Instance-Dependent Attention[1095]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Contrastive Masked Autoencoders[990]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Contrastive Masked Autoencoders[184]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Contrastive Masked Autoencoders[629]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">TransTrack[847]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">TransTrack[2079]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">TransTrack[953]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">Anchor DETR[648]</span>\n",
       "├── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">GCNet[1765]</span>\n",
       "└── <span style=\"color: #ff6464; text-decoration-color: #ff6464\">GCNet[1716]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;100;100mLiterature Research Report\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mSelf-Supervised Learning and Vision Applications[64,80]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced Contrastive Learning Techniques and Minimal Augmentation[11,30]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mConsistency in Visual Recognition[30,1686]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mVisformer[2066]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAttention-Based Learning Techniques[69]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mAdvanced Neural Network Architectures and Self-Supervised Learning[1846,463]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mCross-Batch Memory[1471]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mSelf-Supervised Learning Techniques[260,1083]\u001b[0m\n",
       "│   │   │           └── \u001b[38;2;255;100;100mContrastive Attention and Visual Representation Techniques[1679,489]\u001b[0m\n",
       "│   │   │               └── \u001b[38;2;255;100;100mSLIP[1510]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdvanced Detection and Classification Techniques[1288,978,1696]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mLifted Contrastive Learning and Instance Localization[376,1132,1220]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mSelf-Supervised Learning in Computer Vision[459,245,452]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mView-Agnostic Dense Representation Learning[115,389,1548]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mContrastiveCrop[1960]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mMinimal Augmentation Techniques[1260]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced Masked Image Modeling Techniques and Scalable Learning[36,605,774]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mMasked Feature Learning[96]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mSelf-Supervised Learning Frameworks[763]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mSelf-Supervised Visual Learning[1924]\u001b[0m\n",
       "│   │   │           └── \u001b[38;2;255;100;100mMasked Image Residual Learning[1010]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mMixMIM Methodology and Masked Autoencoder Contrastive Tuning[98]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mMasked Autoencoding Techniques[1131,501]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mSemantic-Guided Masked Autoencoders and Generative Modeling[1726,1660,1873]\u001b[0m\n",
       "│   │   │           └── \u001b[38;2;255;100;100mMAAGE Framework and Its Components[806,1598]\u001b[0m\n",
       "│   │   │               ├── \u001b[38;2;255;100;100mConvNeXt V2 and Mixed Autoencoders[2160]\u001b[0m\n",
       "│   │   │               │   └── \u001b[38;2;255;100;100mSelf-Supervised Visual Representation Learning[1059,677,1200,1265]\u001b[0m\n",
       "│   │   │               │       └── \u001b[38;2;255;100;100mDeepMIM and Pre-Pretraining Techniques[1536]\u001b[0m\n",
       "│   │   │               │           └── \u001b[38;2;255;100;100mPre-Pretraining with MAE[798]\u001b[0m\n",
       "│   │   │               └── \u001b[38;2;255;100;100mSkeletonMAE[1035]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mData Scaling in MIM[1835]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mGenerative Hierarchical Feature[1066,74]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mUnsupervised Domain Generalization[2004,41]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mEnhanced Denoising Techniques[1313,1511]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mKnowledge Distillation Techniques[1733]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mReplace one Branch[1432]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mExtreme-Multi-Patch Self-Supervised Learning[571]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSelf-Attention Mechanisms in Vision Models[582,893,2028]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mAttention Free Transformer[1039]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mVision GNN[330]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSelf-Supervised Video Object Segmentation and MultiSiam[801,1666,1850]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mMultiSiam[2135]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDEtection TRansformer[2154]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDense Teacher[1524]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSelf-Guided Diffusion Models[1456]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mScene Text Detection and Recognition[1618]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mInformation-Theoretic Analysis of Self-Supervised Learning[2026]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAdvanced Visual Recognition and Semi-Supervised Learning Techniques[92,1221]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced Object Detection and Knowledge Distillation Techniques\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdvanced Object Detection Techniques[40,378]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mNamedMask[1649]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mObject Detection and Adaptivity Innovations[936]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mDimension-Decomposition Region Proposal Network (DeRPN)[855]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mObject Detection and Localization Techniques[84]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mAdvanced Object Detection Models[165,2151]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mSOLO[407]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mWeakly Supervised Learning Techniques and Object Detection[1326,555,1669]\u001b[0m\n",
       "│   │   │       │   ├── \u001b[38;2;255;100;100mAdvanced Neural Network Techniques[446,400]\u001b[0m\n",
       "│   │   │       │   └── \u001b[38;2;255;100;100mEnhanced Object Detection Techniques[1884,478]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mSqueezeDet[1576]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mAmbiguity-Resistant Semi-supervised Learning[1740]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mYOsO Framework[697]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mDeep Snake[1204]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mMontage Pre-training[1748]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mCurve Text Detection[736]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mFocal-Stable-DINO[1167]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mCoordConv[1887]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdvanced Feature Extraction in Computer Vision[904,667]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mObject Relation and Representation Learning Techniques[1693]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mRegion-to-Object Representation Learning and YOLOS[898]\u001b[0m\n",
       "│   │   │       │   └── \u001b[38;2;255;100;100mYOLOS[1015]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mAnti-Aliasing in Downsampling Layers[89]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mAdvanced Neural Network Techniques[1752]\u001b[0m\n",
       "│   │   │           ├── \u001b[38;2;255;100;100mRobust Vision Transformer[311]\u001b[0m\n",
       "│   │   │           └── \u001b[38;2;255;100;100mSelective Feature Connection Mechanism[1790]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mGuided Anchoring[545]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mConditional DETR[822,1417,1380]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mBi-Level Optimized Query Slot Attention[21]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mTextScanner[1196]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mScale Normalization for Image Pyramids (SNIP)[819]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mDRAEM[1615]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mJoinDet[405]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mTokens-to-Token Vision Transformers[1659]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mRepFusion[1522]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mFCOS[1837]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mFast Knowledge Distillation[2068]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mUnified Detection Methods\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mUnified Multi-Dataset Detector[80]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdvanced Object Detection Frameworks[91]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mSTAC Framework and ViLD[916]\u001b[0m\n",
       "│   │   │   │   └── \u001b[38;2;255;100;100mViLD[299]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mDetic[1923]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mGrounded Language-Image Pre-training[785]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mMulti-Label Image Recognition[167]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mPix2Seq Framework[1719]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mDeep Seeded Region Growing[896]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mOpen-World Instance Segmentation[1480]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mZero-Shot Semantic Segmentation[1436]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mUnified Panoptic and Open-World Segmentation[768,1657,207]\u001b[0m\n",
       "│   │           ├── \u001b[38;2;255;100;100mInstance Segmentation Frameworks[1363]\u001b[0m\n",
       "│   │           │   └── \u001b[38;2;255;100;100mUnified Segmentation Models[1335]\u001b[0m\n",
       "│   │           │       └── \u001b[38;2;255;100;100mUnsupervised Learning Models[1970]\u001b[0m\n",
       "│   │           │           └── \u001b[38;2;255;100;100mHybrid Task Cascade[914]\u001b[0m\n",
       "│   │           └── \u001b[38;2;255;100;100mCAT: Cascade Detection Transformer[1348]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mQuery-Efficient Meta Attacks[767]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mOpen-Set Face Synthesis and Image Segmentation[987,593]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mAdvanced Image Segmentation Models[723]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mSegGPT[231]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mMaX-DeepLab[1552]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mPyramid Fusion Transformer and Cross Pseudo Supervision[1025]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mCross Pseudo Supervision (CPS)[166,1199]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mVisual Relationship Detection Frameworks[2064]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mSEMPART[594]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mConMIM in Visual Task Applications[687,771]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mContextual Semantic Segmentation Techniques[946,1180]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSimFIR[332]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDeja Vu Memorization[279]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced Attention Mechanisms[1753,2020,1427,541,865]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mRepMLP[310]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mInstance Discrimination with Multi-crop and CutMix[1912]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mOpen-Vocabulary Object Detection[32,217,1614]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mZero-shot Unsupervised Transfer Instance Segmentation[2111]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mAdvanced Object Detection[935]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mOpen-Vocabulary Object Detection[284]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mConditional Variational Autoencoder (CVAE) for Feature Generation[631]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mDeformable DETR[235]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mMixupE[1237]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mIntegral Migration of Pre-trained Vision Transformers[662]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAttention Clusters[1795]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mZero-Shot Visual Recognition[1796]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAttention-Based Fine-Grained Transfer Learning[1859]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced Text Detection Techniques[1971,1489]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mText Detection and Recognition[105]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mSATRN[650]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mMetNet-2[1347]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAttention Mechanisms in NLP[1234]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mFoveaBox[1560]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mImplicit Slot Attention[1783]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mGenerative Models vs. Contrastive Learning in Few-Shot Segmentation[913]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSemantic-SAM[1521]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDeepLIFT[1540]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mMacaron Net[1324]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mSemCo[908]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mMulti-Modal Learning and Hypernetworks[72]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mVision-Language and Instructional Models\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mMultimodal Learning and Optimization Models[52]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mVision-and-Language and Multimodal Models[31]\u001b[0m\n",
       "│   │   │   │   ├── \u001b[38;2;255;100;100mAdvanced Video Models and Human-Object Interaction[617]\u001b[0m\n",
       "│   │   │   │   │   ├── \u001b[38;2;255;100;100mReceptive Field Block in CNN Architectures[1845,1355]\u001b[0m\n",
       "│   │   │   │   │   └── \u001b[38;2;255;100;100mHuman-Object Interaction Detection[1045]\u001b[0m\n",
       "│   │   │   │   ├── \u001b[38;2;255;100;100mAdvanced Attention Mechanisms and Language Modeling[1320]\u001b[0m\n",
       "│   │   │   │   │   ├── \u001b[38;2;255;100;100mDynamic Capsule Attention[960,1590]\u001b[0m\n",
       "│   │   │   │   │   └── \u001b[38;2;255;100;100mVision-Language Pretraining Techniques and VL-BERT[1233,227]\u001b[0m\n",
       "│   │   │   │   │       └── \u001b[38;2;255;100;100mVL-BERT[700]\u001b[0m\n",
       "│   │   │   │   └── \u001b[38;2;255;100;100mGemini Multimodal Models[2040]\u001b[0m\n",
       "│   │   │   │       ├── \u001b[38;2;255;100;100mVisionLLM[1005]\u001b[0m\n",
       "│   │   │   │       └── \u001b[38;2;255;100;100mMultimodal Language-Image Models[572]\u001b[0m\n",
       "│   │   │   │           └── \u001b[38;2;255;100;100mMultimodal AI[2052,291]\u001b[0m\n",
       "│   │   │   │               └── \u001b[38;2;255;100;100mContrastive Captioners[238]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mContext Optimization Techniques[2089,1520,2058]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mOpen-Vocabulary Semantic Segmentation[83]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mUnsupervised Text-to-Image Generation[1799]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mVisual Prompting[1652]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mInstruct Diffusion[216]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mUniversal Representations\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mHuman Visual System's Consistency\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mMachine Vision and Multimodal Learning[72]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mMeta-Transformer[761]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mTemporal Span Proposal Network[342]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdversarial Multi-Task Learning Techniques[2021]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mUnified Image Understanding[130]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mDetection Hub[1172]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mSynthetic Data-driven Learning Techniques[540]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mMulti-Task Self-Training[1717]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mInterpretable Convolutional Neural Networks[928]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mHypernetworks[387]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAdvanced Network Architectures and Visual Representation Techniques[68,77]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced Network Optimization Techniques and Efficient Transformers\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAttention Mechanisms and Global Context in Vision Transformers\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mVision Transformer Architectures and Attention Mechanisms[2046,859,1586,1515]\u001b[0m\n",
       "│   │   │   │   ├── \u001b[38;2;255;100;100mAdvanced Vision Transformers[743,1701,94,0]\u001b[0m\n",
       "│   │   │   │   │   ├── \u001b[38;2;255;100;100mVisual Transformers[1920]\u001b[0m\n",
       "│   │   │   │   │   │   └── \u001b[38;2;255;100;100mDeepViT[781]\u001b[0m\n",
       "│   │   │   │   │   └── \u001b[38;2;255;100;100mSo-ViT[1843]\u001b[0m\n",
       "│   │   │   │   ├── \u001b[38;2;255;100;100mPyramid Vision Transformer v2[1475]\u001b[0m\n",
       "│   │   │   │   └── \u001b[38;2;255;100;100mAttention Mechanisms in Vision Transformers[2103]\u001b[0m\n",
       "│   │   │   │       └── \u001b[38;2;255;100;100mConditional Positional Encodings[1276]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mPale-Shaped Self-Attention and Semi-Supervised Learning[379,1759]\u001b[0m\n",
       "│   │   │   │   └── \u001b[38;2;255;100;100mSemi-MAE[123]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mVision Transformer Architectures[1033]\u001b[0m\n",
       "│   │   │   │   └── \u001b[38;2;255;100;100mVision Transformer Architectures[625]\u001b[0m\n",
       "│   │   │   │       └── \u001b[38;2;255;100;100mViT-Slim[1621]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mExtended Transformer Construction[919]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mEnhanced Local Self-Attention[430]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mActive Memory Models[1068]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mDual PatchNorm[286]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mGlobal Context Networks[445]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mComputational Efficiency in Deep Learning[56]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mEfficient Deep Learning Techniques[1472,23]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mTurbo Training and Efficiency Optimization[1371]\u001b[0m\n",
       "│   │   │           └── \u001b[38;2;255;100;100mBlockDrop[664]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mNetwork Design Methodologies[1365,765]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mUniNet[791]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mUniNet[1688]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdvanced Generative Models[170]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mFixed Point Diffusion Models and MLP Architectures[1940]\u001b[0m\n",
       "│   │   │   │   └── \u001b[38;2;255;100;100mMLP-Mixer[1746]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mBayesian SegNet[872]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mBinary Spherical Quantization[178]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mMacaron Net Architecture[1236,202]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdaptive Graph Convolutional Networks[635]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mMatryoshka Representation Learning[2142]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mEfficient Transformers[735]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mParameter-Free Operations\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdvanced Neural Network Architectures\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mProjection and Rejection Products in Neural Networks[751,639]\u001b[0m\n",
       "│   │   │   ├── \u001b[38;2;255;100;100mGraph HyperNetworks[1781]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mTransformer Set Prediction Network[412]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mVision Transformers[68,1310]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced Visual Representation and Semantic Recognition Techniques[146,1650]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mFlow Capsules and Vision Transformers[1528]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mInception Transformer[433]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mFoveated Vision and Bayesian Segmentation Models[1609]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mBayesian SegNet[1029]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mTransformers in Visual Recognition[2013]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mMultiscale Vision Transformers[204,1168]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mVisual Autoregressive Modeling[973]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdaptive Embedding Gate[1965]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mSemantic Sparsity in Situation Recognition[1082]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mRandomly Wired Neural Networks and Advanced Architectures[1488]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mCutting-edge Neural Network Architectures[1440]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mCORnet-S and Vision Computing[1333]\u001b[0m\n",
       "│   │       │   └── \u001b[38;2;255;100;100mVision Reservoir Computing[396]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mGraph Neural Network Architectures[449,902]\u001b[0m\n",
       "│   │       │   └── \u001b[38;2;255;100;100mGLOM[653]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mX-volution[1282]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mNested Transformers[180]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mCMT and Transferable Architecture Learning[654]\u001b[0m\n",
       "│   │       │   └── \u001b[38;2;255;100;100mTransferable Architecture Learning[329]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mPerceiver IO[1322]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mReNet[1298]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mTemporal Value Modeling and Key Aggregation[1404]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSparse Manifold Transform[195]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mCoordinated Policy Optimization[298]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSelf-Supervised Learning Techniques[264,878]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mComputational Prototype Model[183]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mFNet[256]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mGraphormer[1372]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mVisual Attention Model for Clothing Retrieval[931]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mInvariance Learning in Vision Transformers[925]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100m3D Visual Processing and Human Pose Estimation\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSelf-Attention for Point Clouds\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mPoint Cloud Processing Techniques and Frameworks[99,1663]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mImplicit Autoencoder Framework and Masked Autoencoders[479,117]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mMasked Autoencoders for Point Clouds[1289]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mPosition Encoding and Architecture\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mPoint-BERT for 3D Point Clouds\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mPoint Cloud Processing and Recognition Techniques[1384]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100m3D Point Cloud Recognition Techniques[346,209]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mTransfer Learning and Evaluation[92]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDynamic Graph CNN[1860]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mMixture Model CNNs[578]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mBlendedMVS Dataset[2119]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mUnsupervised Image Segmentation Techniques and Frameworks[1988,1158]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mPointContrast[1014]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mInterHand2.6M Dataset[1454]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDiffusion Model for Dense Visual Prediction[1578]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100m3D Concept Learning and Reasoning Framework[174]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mRIM-Net[811]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSituation Recognition and Grounded Situation Recognition[249]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mGrounded Situation Recognition[1448]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced 3D Object Detection Frameworks[297,1446]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mDense Human Pose Estimation[758]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mImage Processing and Computer Vision Techniques[783]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mReinforcement Learning in Image Captioning and Optimization[1544]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mAdvanced Reinforcement Learning Techniques[65,556,844]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mAdvanced Image Captioning Techniques[1382,823]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mActor-Critic Sequence Training[1061]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mImage Captioning Metrics Optimization\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mDiversity Metrics[1551]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mAdaptive Image Captioning and Neural Frameworks[1354,883]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mNeural Baby Talk[717]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mScene Recognition Techniques[155]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mCNN Text Processing and Transformation Networks\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mGeometry-aware Text Recognition and Augmentation Techniques[933]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mArabic Scene Text Recognition[741]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mFast Oriented Text Spotting[853]\u001b[0m\n",
       "│   │   │       │   └── \u001b[38;2;255;100;100mNeural Architectures for Text Recognition[882,1961,623,696,331,78,2032]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mSTEFANN[25]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mMASTER[655]\u001b[0m\n",
       "│   │   │       ├── \u001b[38;2;255;100;100mLearnable Data Augmentation[1169]\u001b[0m\n",
       "│   │   │       └── \u001b[38;2;255;100;100mManifold Mixup[341]\u001b[0m\n",
       "│   │   ├── \u001b[38;2;255;100;100mEfficient Data Preparation Strategies[95,1089]\u001b[0m\n",
       "│   │   │   └── \u001b[38;2;255;100;100mEdit Probability in Text Recognition[1880,1772]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mSphereFace[1046]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDragonDiffusion[1980]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mFreeDrag[1736]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDensity-Aware Image Restoration[2039]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mImage Restoration Techniques[251]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mAdjacent Aggregation Network[676]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mHierarchical Integration Diffusion Model[938]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mNormalizer-Free ResNets and Harmonic Networks[124]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mHarmonic Networks[443]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mVisual Attention and Interpretability Techniques[62]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mGrad-CAM[949]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mUnsupervised Semantic Segmentation Techniques[1146]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mAdvanced Segmentation Techniques[118,1078]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mSemantic Image Segmentation Framework[619,665]\u001b[0m\n",
       "│   │           └── \u001b[38;2;255;100;100mDense Learning Frameworks[621]\u001b[0m\n",
       "│   │               ├── \u001b[38;2;255;100;100mSoft Proposal Networks and Related Methodologies[364,1767]\u001b[0m\n",
       "│   │               └── \u001b[38;2;255;100;100mEnd-to-End Instance Segmentation with Recurrent Attention[1969]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mHigh-Resolution Image Synthesis Techniques[144,1353]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mLatent Diffusion Models and Applications[862]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mImage Synthesis and Editing Frameworks[129]\u001b[0m\n",
       "│   │           └── \u001b[38;2;255;100;100mHigh-Resolution Image Generation and Neural Network Architectures[1444]\u001b[0m\n",
       "│   │               ├── \u001b[38;2;255;100;100mHigh-Resolution Image Synthesis and Virtual Try-On[368]\u001b[0m\n",
       "│   │               │   └── \u001b[38;2;255;100;100mVITON[1318]\u001b[0m\n",
       "│   │               └── \u001b[38;2;255;100;100mMobile-Former and Visual AutoRegressive Modeling[1852]\u001b[0m\n",
       "│   │                   └── \u001b[38;2;255;100;100mVisual AutoRegressive Modeling[1587]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mLong Short-Term Memory (LSTM) in Sequence Generation[1858]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mGenerative Object Compositing Techniques[1947,1992]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mDynamic Least-to-Most Prompting[440]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSelf-Supervised Vision Transformers[1596]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mAdvanced Visual Transformers[918]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mDynamic Vision Transformer[542]\u001b[0m\n",
       "│   │       ├── \u001b[38;2;255;100;100mVOLO (Vision Outlooker)[728]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mMasked Autoencoders[810]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mVisual Recognition by Request[1296]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mRobust Dense Feature Matching[897]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mContinual Detection Transformer[1705]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mEnhanced Simple Online and Realtime Tracking with Deep Association Metric[35]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mScale Steerable Filters[1129]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mSelf-Supervised In-Context Learning (SINC)[809]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mPosition Information in CNNs and ViTs with Applications in Depth Estimation[1285,2147,224,747]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mCascaded ConvNet for Human Pose Estimation[691]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mGaussian Attention Bias[468]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mMonocular Depth Estimation[1854]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAdvanced Weather Forecasting Technologies[1208,472]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAdvanced Polynomial Activation Functions[1457,1139,769,557]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mYOLOv7[1678]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mNonlocal Echo Dynamics Network[295]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mVision Transformers and Specialized Architectures[577]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mMotion-Guided Global-Local Aggregation Transformer Network[846]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mScore-Based Diffusion Models for Weather Forecasting[126]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mVariational Inference Techniques[975,127]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mDifferentiable Search Index[1097]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mInterpretable and Unified Machine Learning Approaches[1585,2000,1359,1452]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mCNN Interpretability and Object Detection Frameworks[306,705]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mDynamic Head Framework and Interpretability Techniques[703]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mGrad-CAM[2129]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAttribution Priors and Weakly-Supervised Techniques[1730]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mWeakly-Supervised Learning Techniques[1049,1760]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mWeakly Supervised Pre-training with Noisy Labels[1327]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mOptical Flow Guided Feature[531]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mNowcasting Models[690,1114]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mModified LSTM Architectures for Improved Interpretability[2096]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSmoothGrad[1007]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mUnified Modeling Approaches[790]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mFace Clustering on Affinity Graphs[2150]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mTemporal Action Detection and Human-Object Interaction Frameworks[1115,47]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mParallel Point Detection and Matching[2045]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAnyDoor[1793]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mSpatial-Temporal Baseline for VideoHOI[721]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mMonolingual Corpora Integration[1816]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mCoNe[363]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mVideo Understanding and Motion Estimation Technologies[598,305]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mLexicon-Free OCR[1635]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mVideo Visual Relation Detection and Reasoning[627]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mVisual Transformation and Segmentation Techniques[704]\u001b[0m\n",
       "│   │       └── \u001b[38;2;255;100;100mMask-Free Video Instance Segmentation[2014]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mNon-Contrastive Self-Supervised Learning and Efficient CNN Inference[1831]\u001b[0m\n",
       "│   │   └── \u001b[38;2;255;100;100mRecurrent Residual Module[567]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mOmniMotion[1875]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mTask Transfer Across Domains[766]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mBenchmarking Graph Neural Networks[1085]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mBenchmarking Frameworks for GNNs[1092]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mHyperPrompt[1825]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mFeature Denoising[808]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mKnowledge Factorization and Dynamic Prior Knowledge[1219]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mDynamic Prior Knowledge[254]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mSimplex Autoencoders[684]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mFew-Shot Learning Techniques[1070]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAnnotator Learning with GANs[1476]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSANet[1430]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mAuto-Reconstructor Network (ARNet)[1013]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mFew-Shot Learning in Image Classification[1319]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mHardness-Aware Deep Metric Learning[236]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mHardness-Aware Deep Metric Learning[1081]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mTest-Time Batch Normalization[870]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAI Academic Competitiveness[201]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAI Academic Resource Strategies[1482]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAdvanced Deep Learning Techniques[695]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mLookupViT and RepMLP[911]\u001b[0m\n",
       "│       └── \u001b[38;2;255;100;100mImage Recognition and Generation Techniques[1620]\u001b[0m\n",
       "│           └── \u001b[38;2;255;100;100mDiffusion Transformers[1368]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAdvanced Sequence Modeling Techniques[1770]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mAdvanced Sequence Modeling Techniques[12]\u001b[0m\n",
       "│       └── \u001b[38;2;255;100;100mSelf-Critical Sequence Training[744]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAdvanced Semantic Segmentation Techniques[554]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mCold Fusion[87]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mViT-Adapter[461]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mMaskFormer[793]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mGD-CAF Model[1283]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mHuman vs. Machine Attention in Image Captioning[2122]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mSimMIM[756]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mOpen-Vocabulary Semantic Segmentation and Adaptive Techniques[709,361]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mUnsupervised Semantic Segmentation[1738]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mLabel Mask AutoEncoder[1804]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mCommon Object Counting[1241]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mNonparametric Semantic Segmentation[502]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mHierarchical Vision Transformers and Object Detection Techniques[730,860,1927,2124]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mAttentionMask[776]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mArbitrary-Oriented Text Detection[1564]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mMixPro[2084]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mSkip-Attention (SkIPAT)[481]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mEfficient Self-Attention Mechanisms and Architectures in Vision and Speech Recognition[1683]\u001b[0m\n",
       "│   ├── \u001b[38;2;255;100;100mEfficient Transformer Architectures[689,13]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mJoint CTC-Attention Mechanism[59]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mSkip-Attention (SkIPAT) for Reduced Computational Complexity[2038]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mSkip-Attention[1135]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mInstance Segmentation and Simulation[1518]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mCARLA Simulator[359]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mUnsupervised Learning in Image Segmentation[203,1420,1410]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mExtremal Perturbations[380]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mExtremal Perturbations[983]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mExemplar Compression in Incremental Learning[795,1757]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mSparsifiner[1640]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mRotation-Sensitive Text Detection and k-NN Attention[210]\u001b[0m\n",
       "│   └── \u001b[38;2;255;100;100mk-NN Attention[390]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mSparsifiner[1840]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mSparse Instance-Dependent Attention[1095]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mContrastive Masked Autoencoders[990]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mContrastive Masked Autoencoders[184]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mContrastive Masked Autoencoders[629]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mTransTrack[847]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mTransTrack[2079]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mTransTrack[953]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mAnchor DETR[648]\u001b[0m\n",
       "├── \u001b[38;2;255;100;100mGCNet[1765]\u001b[0m\n",
       "└── \u001b[38;2;255;100;100mGCNet[1716]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print as rprint\n",
    "rich_output, term_output = tree.tree_snapshot_colored()\n",
    "print(\"Rich (Jupyter) Output:\")\n",
    "rprint(rich_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ba5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(tree, \"snap.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "677084a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the comprehensive analysis of the paper in the requested format:\n",
      "\n",
      "**1. Main Problem**\n",
      "* **Summary:** The primary issue addressed in this paper is the detection of backdoor attacks in deep neural networks (DNNs), specifically identifying backdoor samples within a poisoned training dataset without prior knowledge of the trigger pattern, backdoored class, or poisoning rate.\n",
      "\n",
      "**2. Keyword Subordination (Logical Positioning)**\n",
      "* **Hierarchical Keywords:**\n",
      "\t1. **Artificial Intelligence**\n",
      "\t2. **Deep Learning**\n",
      "\t3. **Deep Neural Networks (DNNs)**\n",
      "\t4. **Security & Vulnerabilities**\n",
      "\t5. **Backdoor Attacks**\n",
      "\t6. **Backdoor Sample Detection**\n",
      "\t7. **Cognitive Distillation (CD)**\n",
      "\n",
      "**3. Key Contributions**\n",
      "* **Novel Approach:** Cognitive Distillation (CD), a self-supervised method to distill and detect backdoor patterns within images.\n",
      "* **Findings:** Backdoor correlations are simpler than natural correlations, leading to smaller Cognitive Patterns (CPs) in backdoor samples.\n",
      "* **Application:** Effective detection of a wide range of advanced backdoor attacks and potential biases in real-world datasets.\n",
      "\n",
      "**4. Research Question or Hypothesis**\n",
      "* **Central Inquiry:** Can a self-supervised method (Cognitive Distillation) effectively distill the minimal pattern responsible for a model's prediction, and can this approach be leveraged to detect backdoor samples in poisoned datasets?\n",
      "\n",
      "**5. Methods and Approaches**\n",
      "* **Cognitive Distillation (CD):** An optimization process that extracts a minimal pattern (Cognitive Pattern, CP) from an input image, ensuring the model's output remains unchanged.\n",
      "* **Techniques:**\n",
      "\t+ Input mask optimization\n",
      "\t+ Total Variation loss for smooth masks\n",
      "\t+ Random noise replacement for non-useful pixels\n",
      "* **Frameworks/Models:** PyTorch, various DNN architectures (e.g., ResNet-18, VGG-16, MobileNetV2)\n",
      "\n",
      "**6. Validation or Evaluation**\n",
      "* **Experiments:**\n",
      "\t+ Detection performance against 12 advanced backdoor attacks\n",
      "\t+ Evaluation on 3 datasets (CIFAR-10, GTSRB, ImageNet subset)\n",
      "\t+ Comparison with 5 state-of-the-art backdoor sample detection methods\n",
      "* **Metrics:** Area Under the ROC Curve (AUROC), True Rejection Rate (TRR), False Acceptance Rate (FAR)\n",
      "\n",
      "**7. Datasets or Experimental Setup**\n",
      "* **Datasets:** CIFAR-10, GTSRB, ImageNet subset (200 classes)\n",
      "* **Models:** 6 DNN architectures (ResNet-18, VGG-16, Pre Activation Res Net 101, MobileNetV2, GoogLeNet, EfficientNet-b0)\n",
      "* **Attack Configurations:** 12 backdoor attacks with varying trigger patterns and poisoning rates\n",
      "\n",
      "**8. Main Findings**\n",
      "* **Detection Performance:** CD outperforms baselines in detecting backdoor samples across most attacks and datasets.\n",
      "* **Robustness:** CD shows robust detection under different poisoning rates.\n",
      "* **Bias Detection:** CD can identify potential biases in real-world datasets (e.g., CelebA).\n",
      "\n",
      "**9. Comparison to Related Work**\n",
      "* **Advantages over Baselines:**\n",
      "\t+ Self-supervised nature\n",
      "\t+ No requirement for class labels or prior knowledge of trigger patterns\n",
      "\t+ Effective against a wide range of attacks\n",
      "* **Differences from Similar Methods (LIME, Meaningful Perturbation, Neural Cleanse):**\n",
      "\t+ Self-supervision\n",
      "\t+ Focus on minimal sufficient pattern for any predicted class\n",
      "\t+ Computational complexity independent of the number of classes\n",
      "\n",
      "**10. Limitations**\n",
      "* **Adaptive Attacks:** Potential vulnerability to attackers manipulating the training objective or trigger patterns to evade detection.\n",
      "* **Computational Cost:** Optimization process for each sample might be computationally intensive.\n",
      "\n",
      "**11. Future Directions**\n",
      "* **Adaptive Attack Countermeasures:** Developing strategies to counter training-objective adaptive attacks.\n",
      "* **Extension to Other Domains:** Applying Cognitive Distillation to detect biases or backdoors in other types of machine learning models or datasets.\n",
      "\n",
      "**12. Abstract**\n",
      "* **[Repeated from the Paper]**\n",
      "This paper proposes a simple method to distill and detect backdoor patterns within an image: Cognitive Distillation (CD). ... [Full abstract as provided in the paper content].\n"
     ]
    }
   ],
   "source": [
    "print(contents[990].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca8be16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content(id_address=990, content=\"Here is the comprehensive analysis of the paper in the requested format:\\n\\n**1. Main Problem**\\n* **Summary:** The primary issue addressed in this paper is the detection of backdoor attacks in deep neural networks (DNNs), specifically identifying backdoor samples within a poisoned training dataset without prior knowledge of the trigger pattern, backdoored class, or poisoning rate.\\n\\n**2. Keyword Subordination (Logical Positioning)**\\n* **Hierarchical Keywords:**\\n\\t1. **Artificial Intelligence**\\n\\t2. **Deep Learning**\\n\\t3. **Deep Neural Networks (DNNs)**\\n\\t4. **Security & Vulnerabilities**\\n\\t5. **Backdoor Attacks**\\n\\t6. **Backdoor Sample Detection**\\n\\t7. **Cognitive Distillation (CD)**\\n\\n**3. Key Contributions**\\n* **Novel Approach:** Cognitive Distillation (CD), a self-supervised method to distill and detect backdoor patterns within images.\\n* **Findings:** Backdoor correlations are simpler than natural correlations, leading to smaller Cognitive Patterns (CPs) in backdoor samples.\\n* **Application:** Effective detection of a wide range of advanced backdoor attacks and potential biases in real-world datasets.\\n\\n**4. Research Question or Hypothesis**\\n* **Central Inquiry:** Can a self-supervised method (Cognitive Distillation) effectively distill the minimal pattern responsible for a model's prediction, and can this approach be leveraged to detect backdoor samples in poisoned datasets?\\n\\n**5. Methods and Approaches**\\n* **Cognitive Distillation (CD):** An optimization process that extracts a minimal pattern (Cognitive Pattern, CP) from an input image, ensuring the model's output remains unchanged.\\n* **Techniques:**\\n\\t+ Input mask optimization\\n\\t+ Total Variation loss for smooth masks\\n\\t+ Random noise replacement for non-useful pixels\\n* **Frameworks/Models:** PyTorch, various DNN architectures (e.g., ResNet-18, VGG-16, MobileNetV2)\\n\\n**6. Validation or Evaluation**\\n* **Experiments:**\\n\\t+ Detection performance against 12 advanced backdoor attacks\\n\\t+ Evaluation on 3 datasets (CIFAR-10, GTSRB, ImageNet subset)\\n\\t+ Comparison with 5 state-of-the-art backdoor sample detection methods\\n* **Metrics:** Area Under the ROC Curve (AUROC), True Rejection Rate (TRR), False Acceptance Rate (FAR)\\n\\n**7. Datasets or Experimental Setup**\\n* **Datasets:** CIFAR-10, GTSRB, ImageNet subset (200 classes)\\n* **Models:** 6 DNN architectures (ResNet-18, VGG-16, Pre Activation Res Net 101, MobileNetV2, GoogLeNet, EfficientNet-b0)\\n* **Attack Configurations:** 12 backdoor attacks with varying trigger patterns and poisoning rates\\n\\n**8. Main Findings**\\n* **Detection Performance:** CD outperforms baselines in detecting backdoor samples across most attacks and datasets.\\n* **Robustness:** CD shows robust detection under different poisoning rates.\\n* **Bias Detection:** CD can identify potential biases in real-world datasets (e.g., CelebA).\\n\\n**9. Comparison to Related Work**\\n* **Advantages over Baselines:**\\n\\t+ Self-supervised nature\\n\\t+ No requirement for class labels or prior knowledge of trigger patterns\\n\\t+ Effective against a wide range of attacks\\n* **Differences from Similar Methods (LIME, Meaningful Perturbation, Neural Cleanse):**\\n\\t+ Self-supervision\\n\\t+ Focus on minimal sufficient pattern for any predicted class\\n\\t+ Computational complexity independent of the number of classes\\n\\n**10. Limitations**\\n* **Adaptive Attacks:** Potential vulnerability to attackers manipulating the training objective or trigger patterns to evade detection.\\n* **Computational Cost:** Optimization process for each sample might be computationally intensive.\\n\\n**11. Future Directions**\\n* **Adaptive Attack Countermeasures:** Developing strategies to counter training-objective adaptive attacks.\\n* **Extension to Other Domains:** Applying Cognitive Distillation to detect biases or backdoors in other types of machine learning models or datasets.\\n\\n**12. Abstract**\\n* **[Repeated from the Paper]**\\nThis paper proposes a simple method to distill and detect backdoor patterns within an image: Cognitive Distillation (CD). ... [Full abstract as provided in the paper content].\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f5396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4258635c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content_metadata': {'content_id_address': 65}, 'is_used_for_update': True}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[\"Image Captioning and Scene Text Recognition\"][\"Reinforcement Learning in Image Captioning\"].represented_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1702ae0",
   "metadata": {
    "code_folding": [
     0,
     44
    ]
   },
   "outputs": [],
   "source": [
    "def export_to_json(self, indent: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Export the tree structure to a JSON string.\n",
    "\n",
    "    Args:\n",
    "        indent (int): Number of spaces for JSON indentation\n",
    "\n",
    "    Returns:\n",
    "        str: JSON string representation of the tree\n",
    "    \"\"\"\n",
    "    def _build_dict(node: NodeStructure,depth=0) -> Dict:\n",
    "        \"\"\"Helper method to build dictionary representation of the tree\"\"\"\n",
    "        # Base node attributes\n",
    "        represent_list = node.represented_pool\n",
    "        represents = [t['content_metadata']['content_id_address'] for t in represent_list]\n",
    "        node_dict = {\n",
    "            \"concept_key_word\": node.key,\n",
    "            \"concept_abstract\": node.concept_abstract,\n",
    "            \"concept_represents\":represents ,\n",
    "        }\n",
    "\n",
    "        # Handle children based on level\n",
    "        if node.children:\n",
    "            # Determine the sub-concept level\n",
    "            sub_concept_key = f\"sub_concept_{depth + 1}\"\n",
    "            node_dict[sub_concept_key] = [_build_dict(child,depth+1) for child in node.children]\n",
    "        else:\n",
    "            # Leaf nodes should have an empty sub_concept array\n",
    "            sub_concept_key = f\"sub_concept_{depth + 1}\"\n",
    "            node_dict[sub_concept_key] = []\n",
    "\n",
    "        return node_dict\n",
    "\n",
    "    # Build the full JSON structure\n",
    "    json_dict = {\n",
    "        \"title\": self.value.get(\"title\", \"Untitled\"),\n",
    "        \"area\": self.value.get(\"area\", \"\"),\n",
    "        \"idea_paradigm\": self.value.get(\"idea_paradigm\", \"\"),\n",
    "        \"root_level_concept\": [_build_dict(child,0) for child in self.children]\n",
    "    }\n",
    "\n",
    "    # Convert to JSON string with specified indentation\n",
    "    return json.dumps(json_dict, indent=indent)\n",
    "\n",
    "def save_to_json(self, filepath: str, indent: int = 4) -> None:\n",
    "    \"\"\"\n",
    "    Save the tree structure to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the output JSON file\n",
    "        indent (int): Number of spaces for JSON indentation\n",
    "    \"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(self.export_to_json(indent=indent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b626ef",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6003904a924dd8add7e841548c3481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Literature Research Report -> Efficient and Scalable Network Architectures -> Dynamic Token Sparsification -> Token Importance Scoring and Attention Masking -> New Child\n",
      "<- Action: ADD at Token Importance Scoring and Attention Mechanisms\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Consistency-Based SSL -> New Child\n",
      "<- Action: ADD at Consistency-Based Self-Supervised Learning\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors -> DINO and Contrastive Denoising Training\n",
      "<- Action: ADD at DINO and Object Detection Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Consistency-Based Self-Supervised Learning -> Nearest-Neighbor Contrastive Learning -> New Child\n",
      "<- Action: ADD at Nearest-Neighbor Contrastive Learning and Self-Supervised Learning\n",
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition -> Reinforcement Learning in Image Captioning -> New Child\n",
      "<- Action: ADD at Reinforcement Learning in Image Captioning and Object Relation Modeling\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> NUWA Multimodal Pre-trained Model -> New Child\n",
      "<- Action: ADD at Multimodal Pre-trained Models\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Contrastive Attention-Supervised Tuning (CAST) -> New Child\n",
      "<- Action: UPDATE at Contrastive Attention-Supervised Tuning (CAST)\n",
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors -> DINO and Object Detection Techniques -> Plain DETR Variants -> New Child\n",
      "<- Action: ADD at DETR Object Detection Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Unified Multi-Dataset Detector -> New Child\n",
      "<- Action: ADD at Unified Multi-Dataset Detection and Semantic Segmentation\n",
      "-> Literature Research Report -> Efficient and Scalable Network Architectures -> Parameter-Free Operations -> Hybrid Architectures -> New Child\n",
      "<- Action: ADD at Hybrid Architectures and Convolution Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Unified Multi-Dataset Detection and Semantic Segmentation -> Open-Vocabulary Semantic Segmentation -> New Child\n",
      "<- Action: ADD at Open-Vocabulary Semantic Segmentation and Universal Architectures\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Universal Representations -> Machine Vision and Domain Generalization -> New Child\n",
      "<- Action: ADD at Machine Vision and Compositional Learning\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors -> Fitness NMS and Bounded IoU Loss -> New Child\n",
      "<- Action: ADD at Object Detection Optimization\n",
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition -> Attention-Based Scene Text Recognition\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Universal Representations -> Machine Vision and Compositional Learning -> Compositional Zero-Shot Learning\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection and Classification -> New Child\n",
      "<- Action: ADD at Decoupling Detection, Classification, and Object-Centric Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> Masked Feature Prediction (MaskFeat) -> New Child\n",
      "<- Action: ADD at Masked Feature Prediction and Autoencoder Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> Multimodal Pre-trained Models -> Contrastive Captioners -> New Child\n",
      "<- Action: ADD at Vision-Language Models\n",
      "-> Literature Research Report -> Image Captioning and Scene Text Recognition -> Attention-Based Scene Text Recognition\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM)\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Unified Multi-Dataset Detection and Semantic Segmentation -> Open-Vocabulary Semantic Segmentation and Universal Architectures -> Masked-attention Mask Transformer -> New Child\n",
      "<- Action: ADD at Masked-attention Mask Transformer and PACO Dataset\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors -> Object Detection Optimization -> Dynamic Anchor Shape Learning -> New Child\n",
      "<- Action: ADD at Dynamic Anchor Shape Learning and Scale-Transferrable Detection\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> Open-Vocabulary Semantic Segmentation -> New Child\n",
      "<- Action: ADD at Open-Vocabulary Semantic Segmentation and Region-Level Understanding\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> Multimodal Pre-trained Models -> Vision-Language Models -> InstructBLIP -> New Child\n",
      "<- Action: ADD at Vision-Language Models\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Decoupling Detection, Classification, and Object-Centric Learning -> Unsupervised Object-Centric Learning -> New Child\n",
      "<- Action: ADD at Object-Centric Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> Masked Feature Prediction and Autoencoder Techniques -> Masked Autoencoder Contrastive Tuning (MAE-CT) -> New Child\n",
      "<- Action: ADD at Masked Autoencoder Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detectors\n",
      "<- Action: ADD at End-to-End Object Detection and Tracking\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Universal Representations -> Machine Vision and Compositional Learning -> Compositional Zero-Shot Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection and Tracking -> New Child\n",
      "<- Action: ADD at End-to-End Object Detection, Tracking, and Keypoint Detection\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection, Tracking, and Keypoint Detection -> New Child\n",
      "<- Action: ADD at End-to-End Object Detection, Tracking, Keypoint Detection, and Self-Supervised Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM)\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Consistency-Based Self-Supervised Learning -> Nearest-Neighbor Contrastive Learning and Self-Supervised Learning -> Part-Aware Self-Supervised Pre-Training -> New Child\n",
      "<- Action: ADD at Part-Aware Self-Supervised Learning\n",
      "-> Literature Research Report -> Efficient and Scalable Network Architectures -> Dynamic Token Sparsification -> Token Importance Scoring and Attention Mechanisms -> Focal Self-Attention -> New Child\n",
      "<- Action: ADD at Vision Transformers\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection, Tracking, Keypoint Detection, and Self-Supervised Learning\n",
      "-> Literature Research Report -> Object Detection Innovations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning -> Consistency-Based Self-Supervised Learning -> Nearest-Neighbor Contrastive Learning and Self-Supervised Learning -> Part-Aware Self-Supervised Learning -> Conditional Augmentation-aware Self-supervised Learning -> New Child\n",
      "<- Action: ADD at Conditional Augmentation-aware Self-supervised Learning and Adaptive Spatial Aggregation\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection, Tracking, Keypoint Detection, and Self-Supervised Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Object Detection Innovations -> Multi-Dataset and Unified Detectors -> Unified Multi-Dataset Detection and Semantic Segmentation -> Open-Vocabulary Semantic Segmentation and Universal Architectures -> Masked-attention Mask Transformer and PACO Dataset -> PACO -> New Child\n",
      "<- Action: UPDATE at PACO with MViT Integration\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> Masked Feature Prediction and Autoencoder Techniques -> Masked Autoencoder Techniques -> Semi-MAE -> New Child\n",
      "<- Action: UPDATE at Semi-MAE with TiTok\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Contrastive Learning\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Multi-Modal Representations and Learning -> Vision-Language Models -> Open-Vocabulary Semantic Segmentation and Region-Level Understanding -> TAP Model -> New Child\n",
      "<- Action: ADD at TAP Model and Video Captioning Techniques\n",
      "-> Literature Research Report -> Object Detection Innovations -> End-to-End Object Detection, Tracking, Keypoint Detection, and Self-Supervised Learning -> Object Detection Optimization -> Dynamic Anchor Shape Learning and Scale-Transferrable Detection\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques -> Masked Image Modeling (MIM) -> Masked Feature Prediction and Autoencoder Techniques -> Masked Autoencoder Techniques -> Semi-MAE with TiTok -> New Child\n",
      "<- Action: ADD at Semi-MAE and Attention-Guided Masking\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n",
      "-> Literature Research Report -> Self-Supervised Learning (SSL) Techniques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7ff2d843a050>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhangtianning.di/anaconda3/envs/camel2/lib/python3.10/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tree\u001b[38;5;241m.\u001b[39mbackpropagate()\n",
      "File \u001b[0;32m~/projects/TreeAgentKnowledge/tree_operations.py:312\u001b[0m, in \u001b[0;36mNodeTree.dispatch_content\u001b[0;34m(self, content, start_node)\u001b[0m\n\u001b[1;32m    309\u001b[0m path \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     decision:DispatchDecision \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dispatch_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m#print(decision)\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     path\u001b[38;5;241m.\u001b[39mappend(current_node\u001b[38;5;241m.\u001b[39mkey)\n",
      "File \u001b[0;32m~/projects/TreeAgentKnowledge/tree_operations.py:120\u001b[0m, in \u001b[0;36mNodeAgent.make_dispatch_decision\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make dispatch decision for content\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# message = AgentMessage(\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#         msg_type=MessageType.DISPATCH,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#         content=content,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#decision = self.generate_relevant_decision_via_simple_LLM(content)\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_relevant_decision_via_json_output_LLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# next_message=AgentMessage(\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#         msg_type=MessageType.DISPATCH,\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#         content={\"decision\": decision, \"original_content\": message.content},\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#         sender=self.key,\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#         receiver=message.sender\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decision\n",
      "File \u001b[0;32m~/projects/TreeAgentKnowledge/tree_operations.py:104\u001b[0m, in \u001b[0;36mNodeAgent.generate_relevant_decision_via_json_output_LLM\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mGenerate a decision about content relevance using LLM\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mThe simple means we have not hard structured the output in json format. Usually, structure json can be required by \u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m- `json_decoder` in sglang/outlines\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m- set `json_format` in openai api\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m user_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_relevant_decision_prompt(content)\n\u001b[0;32m--> 104\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(response\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;241m.\u001b[39mcontent) \u001b[38;5;66;03m## <-- make sure the response is a valid json\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbasement_polish\u001b[39m(string):\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/camel/agents/chat_agent.py:388\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, input_message, output_schema)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecord_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: BaseMessage) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Records the externally provided message into the agent memory as if\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    it were an answer of the :obj:`ChatAgent` from the backend. Currently,\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    the choice of the critic is submitted with this method.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m        message (BaseMessage): An external message to be recorded in the\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m            memory.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_memory(message, OpenAIBackendRole\u001b[38;5;241m.\u001b[39mASSISTANT)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/camel/agents/chat_agent.py:676\u001b[0m, in \u001b[0;36m_step_model_response\u001b[0;34m(self, openai_messages, num_tokens)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_token_exceed(\n\u001b[1;32m    671\u001b[0m         e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m], tool_call_records, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens_exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    672\u001b[0m     )\n\u001b[1;32m    674\u001b[0m (\n\u001b[1;32m    675\u001b[0m     response,\n\u001b[0;32m--> 676\u001b[0m     output_messages,\n\u001b[1;32m    677\u001b[0m     finish_reasons,\n\u001b[1;32m    678\u001b[0m     usage_dict,\n\u001b[1;32m    679\u001b[0m     response_id,\n\u001b[1;32m    680\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_model_response(openai_messages, num_tokens)\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tools_added()\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mtool_calls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    686\u001b[0m ):\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/camel/utils/commons.py:271\u001b[0m, in \u001b[0;36mapi_keys_required.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_api_key\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m missing_environment_keys\n\u001b[1;32m    267\u001b[0m ):\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing API keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_environment_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     )\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/camel/models/openai_model.py:112\u001b[0m, in \u001b[0;36mOpenAIModel.run\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 112\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/resources/chat/completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/openai/_base_client.py:983\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    989\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http_proxy.py:344\u001b[0m, in \u001b[0;36mTunnelHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m HTTP11Connection(\n\u001b[1;32m    338\u001b[0m                 origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_origin,\n\u001b[1;32m    339\u001b[0m                 stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    340\u001b[0m                 keepalive_expiry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keepalive_expiry,\n\u001b[1;32m    341\u001b[0m             )\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/ssl.py:1258\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1256\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1257\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/camel2/lib/python3.10/ssl.py:1131\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,filename in enumerate(tqdm(os.listdir(Content_root))):\n",
    "    filepath = os.path.join(Content_root,filename )\n",
    "    with open(filepath,'r') as f:\n",
    "        content = f.read()\n",
    "    tree.dispatch_content(content)\n",
    "    tree.backpropagate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1356d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = tree.children_map['Self-Supervised Learning (SSL) Techniques'].children_map['Masked Image Modeling (MIM)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e405771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "prompt   = self.build_node_action_decision_prompt(content)\n",
    "response = self.step(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0345fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = eval(response.msg.content)\n",
    "details_models = {\"ADD\": AddActionDetails,\"MERGE\": MergeActionDetails}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932de2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_decision = details_models[response['action']](**response['details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8afc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddActionDetails(new_concept_key_word='ConMIM', new_concept_abstract='ConMIM is a novel masked image modeling approach that leverages contrastive learning for self-supervised pre-training of Vision Transformers, eliminating the need for pre-learned image tokenizers and introducing asymmetric designs to enhance denoising mechanisms.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956960eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95004e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self._execute_add_action(action_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee2e4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept Key: Masked Image Modeling (MIM)\n",
      "Concept Description: MIM focuses on predicting missing parts of the input data, often leading to improved understanding of the data structure and better feature extraction.\n",
      "Sub Concepts:\n",
      "    - [Masked Feature Prediction (MaskFeat)]:[Uses masked input sequences and predicts specific features such as HOG to enhance learning efficiency.]\n",
      "    - [MixMIM Methodology]:[A method that replaces MASK symbols with tokens from another image to improve training efficiency and generalization capabilities.]\n",
      "    - [ConMIM]:[ConMIM is a novel masked image modeling approach that leverages contrastive learning for self-supervised pre-training of Vision Transformers, eliminating the need for pre-learned image tokenizers and introducing asymmetric designs to enhance denoising mechanisms.]\n"
     ]
    }
   ],
   "source": [
    "print(self.text_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af951fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_node = self.children_map['ConMIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd379b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts.node_action.update_prompt_after_add_action import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a34d63f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f11b7393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept Key: Masked Image Modeling (MIM)\n",
      "Concept Description: MIM focuses on predicting missing parts of the input data, often leading to improved understanding of the data structure and better feature extraction.\n",
      "Sub Concepts:\n",
      "    - [Masked Feature Prediction (MaskFeat)]:[Uses masked input sequences and predicts specific features such as HOG to enhance learning efficiency.]\n",
      "    - [MixMIM Methodology]:[A method that replaces MASK symbols with tokens from another image to improve training efficiency and generalization capabilities.]\n",
      "    - [ConMIM]:[ConMIM is a novel masked image modeling approach that leverages contrastive learning for self-supervised pre-training of Vision Transformers, eliminating the need for pre-learned image tokenizers and introducing asymmetric designs to enhance denoising mechanisms.]\n"
     ]
    }
   ],
   "source": [
    "print(self.text_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48e8a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response=  self.step(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b224f664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_concept_abstract': 'Masked Image Modeling (MIM) focuses on predicting missing parts of the input data, often leading to improved understanding of the data structure and better feature extraction. This includes techniques such as Masked Feature Prediction and MixMIM Methodology, as well as the novel ConMIM approach, which leverages contrastive learning for self-supervised pre-training of Vision Transformers, enhancing denoising mechanisms and eliminating the need for pre-learned image tokenizers.',\n",
       " 'new_concept_key_word': 'Advanced Masked Image Modeling'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(response.msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2d86d0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision='LOWER' next_position='Self-Supervised Learning (SSL) Techniques' reasoning=\"The content specifically addresses advancements in self-supervised visual representation learning, particularly focusing on Vision Transformers and contrastive learning methods. This aligns closely with the child node 'Self-Supervised Learning (SSL) Techniques', making it suitable for placement there as it elaborates on specific techniques and innovations within that domain.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision='LOWER' next_position='Masked Image Modeling (MIM)' reasoning='The new content specifically addresses improvements in self-supervised visual representation learning using Vision Transformers, focusing on the effectiveness of contrastive learning in masked image modeling. This aligns directly with the sub-concept of Masked Image Modeling (MIM), making it suitable for placement under this child node.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision='LOWER' next_position='MixMIM Methodology' reasoning='The content discusses a novel method, ConMIM, which is a specific approach within the broader context of Masked Image Modeling (MIM). This method directly relates to the MixMIM Methodology as it proposes enhancements to the training efficiency and generalization capabilities of Vision Transformers, making it suitable for this child node.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.gptsapi.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision='LOWER' next_position='ConMIM Method' reasoning='The new content provides detailed insights into the ConMIM methodology, including its problem statement, key contributions, methods, and findings. This aligns closely with the specific focus of the MixMIM Methodology node, making it suitable for a child node dedicated to the ConMIM method.'\n",
      "=== create new node, then we need genereate the concept_key_word and concept_abstract ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Literature Research Report',\n",
       " 'Self-Supervised Learning (SSL) Techniques',\n",
       " 'Masked Image Modeling (MIM)',\n",
       " 'MixMIM Methodology',\n",
       " 'ConMIM Method']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.dispatch_content(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camel_good",
   "language": "python",
   "name": "camel_good"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
